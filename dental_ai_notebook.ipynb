{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am going to use `gemini-1.5-flash-8b` using google api key. One of the best features of Google's Gemini API is its intuitive chat handling. Unlike other LLM APIs, you don't need to specify roles like \"user\" or \"assistant\" - Gemini handles this automatically. Other APIs, such as OpenAI, often require something like:\n",
    "```python\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant\"},\n",
    "    {\"role\": \"user\", \"content\": \"Hello\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Hi there!\"},\n",
    "    {\"role\": \"user\", \"content\": \"How are you?\"}\n",
    "]\n",
    "```\n",
    "\n",
    "We can also utilize the libraries like `gpt4all`  to load LLM modules via the Python SDK and API (without downloading locally, though local usage is supported).\n",
    "\n",
    "Reference: [GPT4All Documentation](https://docs.gpt4all.io/index.html)\n",
    "\n",
    "Install `gpt4all`: pip install gpt4all\n",
    "\n",
    "Other frameworks to run LLM locally: [Frameworks Guide](https://www.datacamp.com/tutorial/run-llms-locally-tutorial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import required libraries and modules\n",
    "import os\n",
    "from dotenv import load_dotenv         # pip install python-dotenv\n",
    "import google.generativeai as genai    # pip install google-generativeai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " > tts_models/en/ljspeech/tacotron2-DDC is already downloaded.\n",
      " > vocoder_models/en/ljspeech/hifigan_v2 is already downloaded.\n",
      " > Using model: Tacotron2\n",
      " > Setting up Audio Processor...\n",
      " | > sample_rate:22050\n",
      " | > resample:False\n",
      " | > num_mels:80\n",
      " | > log_func:np.log\n",
      " | > min_level_db:-100\n",
      " | > frame_shift_ms:None\n",
      " | > frame_length_ms:None\n",
      " | > ref_level_db:20\n",
      " | > fft_size:1024\n",
      " | > power:1.5\n",
      " | > preemphasis:0.0\n",
      " | > griffin_lim_iters:60\n",
      " | > signal_norm:False\n",
      " | > symmetric_norm:True\n",
      " | > mel_fmin:0\n",
      " | > mel_fmax:8000.0\n",
      " | > pitch_fmin:1.0\n",
      " | > pitch_fmax:640.0\n",
      " | > spec_gain:1.0\n",
      " | > stft_pad_mode:reflect\n",
      " | > max_norm:4.0\n",
      " | > clip_norm:True\n",
      " | > do_trim_silence:True\n",
      " | > trim_db:60\n",
      " | > do_sound_norm:False\n",
      " | > do_amp_to_db_linear:True\n",
      " | > do_amp_to_db_mel:True\n",
      " | > do_rms_norm:False\n",
      " | > db_level:None\n",
      " | > stats_path:None\n",
      " | > base:2.718281828459045\n",
      " | > hop_length:256\n",
      " | > win_length:1024\n",
      " > Model's reduction rate `r` is set to: 1\n",
      " > Vocoder Model: hifigan\n",
      " > Setting up Audio Processor...\n",
      " | > sample_rate:22050\n",
      " | > resample:False\n",
      " | > num_mels:80\n",
      " | > log_func:np.log\n",
      " | > min_level_db:-100\n",
      " | > frame_shift_ms:None\n",
      " | > frame_length_ms:None\n",
      " | > ref_level_db:20\n",
      " | > fft_size:1024\n",
      " | > power:1.5\n",
      " | > preemphasis:0.0\n",
      " | > griffin_lim_iters:60\n",
      " | > signal_norm:False\n",
      " | > symmetric_norm:True\n",
      " | > mel_fmin:0\n",
      " | > mel_fmax:8000.0\n",
      " | > pitch_fmin:1.0\n",
      " | > pitch_fmax:640.0\n",
      " | > spec_gain:1.0\n",
      " | > stft_pad_mode:reflect\n",
      " | > max_norm:4.0\n",
      " | > clip_norm:True\n",
      " | > do_trim_silence:False\n",
      " | > trim_db:60\n",
      " | > do_sound_norm:False\n",
      " | > do_amp_to_db_linear:True\n",
      " | > do_amp_to_db_mel:True\n",
      " | > do_rms_norm:False\n",
      " | > db_level:None\n",
      " | > stats_path:None\n",
      " | > base:2.718281828459045\n",
      " | > hop_length:256\n",
      " | > win_length:1024\n",
      " > Generator Model: hifigan_generator\n",
      " > Discriminator Model: hifigan_discriminator\n",
      "Removing weight norm...\n",
      " > Text splitted to sentences.\n",
      "['\"Hi, this is AI Assistant calling from Raj Dental Clinic. I\\'m calling to verify insurance benefits for our patient, Mr. Amit Sharma.\"']\n",
      " > Processing time: 12.391620635986328\n",
      " > Real-time factor: 1.2585453746752642\n"
     ]
    }
   ],
   "source": [
    "load_dotenv()\n",
    "api_key = os.getenv('GOOGLE_API_KEY')\n",
    "office_name = 'Raj dental clinic'\n",
    "system_prompt = f\"\"\"You are a dental office AI assistant specializing in insurance verification calls from {office_name}.\n",
    "Your role is to:\n",
    "1. Verify patient insurance coverage and benefits\n",
    "2. Gather information about deductibles, maximums, and coverage percentages\n",
    "3. Handle insurance-related inquiries professionally and concisely\n",
    "4. Respond in a conversational yet professional manner\n",
    "\n",
    "Start the call with a professional introduction like: \n",
    "\"Hi, this is [Assistant Name] calling from {office_name}. I'm calling to verify insurance benefits for our patient [Patient Name].\"\n",
    "\"\"\"\n",
    "genai.configure(api_key=api_key)\n",
    "        \n",
    "model = genai.GenerativeModel(model_name = 'gemini-1.5-flash-8b')      \n",
    "chat = model.start_chat(history = [])\n",
    "start_conversation = chat.send_message(system_prompt).text\n",
    "\n",
    "# convert above text to speech then audio\n",
    "import base64\n",
    "from io import BytesIO\n",
    "import numpy as np\n",
    "import scipy.io\n",
    "from TTS.api import TTS\n",
    "import simpleaudio as sa\n",
    "import logging, warnings\n",
    "from scipy.io import wavfile\n",
    "import torch\n",
    "\n",
    "\n",
    "# Suppress warnings and verbose output\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "logging.getLogger(\"TTS.utils.io\").setLevel(logging.ERROR)\n",
    "logging.getLogger(\"TTS.utils.audio\").setLevel(logging.ERROR)\n",
    "logging.getLogger(\"TTS\").setLevel(logging.ERROR)\n",
    "\n",
    "# Initialize TTS with optimization\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "tts = TTS(\n",
    "    model_name=\"tts_models/en/ljspeech/tacotron2-DDC\",\n",
    "    progress_bar = False\n",
    ").to(device)\n",
    "\n",
    "# Convert text to speech and save to file\n",
    "# tts.tts_to_file(text = start_conversation, speed = 2.0, file_path = 'output.wav')\n",
    "\n",
    "# encoding function\n",
    "# def encode_wav(wav):\n",
    "#     wav = np.array(wav)\n",
    "#     wav_norm = wav * (32767 / max(0.01, np.max(np.abs(wav))))\n",
    "#     wav_norm = wav_norm.astype(np.int16)\n",
    "#     wav_obj = sa.WaveObject(wav_norm, 1, 2, 22050)\n",
    "#     return wav_obj\n",
    "\n",
    "\n",
    "# Generate speech\n",
    "wav = tts.tts(text = start_conversation, speed = 1.2)\n",
    "\n",
    "# Convert to numpy array and normalize\n",
    "wav = np.array(wav)\n",
    "wav_norm = wav * (32767 / max(0.01, np.max(np.abs(wav))))\n",
    "wav_norm = wav_norm.astype(np.int16)\n",
    "\n",
    "# Create and play audio object\n",
    "wav_obj = sa.WaveObject(wav_norm, 1, 2, 22050)\n",
    "play_obj = wav_obj.play()\n",
    "play_obj.wait_done()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Speech to Text Conversion Using Whisper and SpeechRecognition\n",
    "\n",
    "This code outlines the steps to convert spoken language into text using Python libraries: `SpeechRecognition` and `Whisper`.\n",
    "\n",
    "#### Libraries to Install\n",
    "\n",
    "```bash\n",
    "pip install SpeechRecognition\n",
    "pip install openai-whisper\n",
    "pip install pyaudio  # Required for microphone input with SpeechRecognition\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjusting for ambient noise... Please wait...\n",
      "\n",
      "Listening... Speak now!\n",
      "Processing speech with Whisper...\n",
      "You said:  Hi, what's up?\n"
     ]
    }
   ],
   "source": [
    "# Speak into mic and convert it to text\n",
    "# pip install SpeechRecognition\n",
    "# pip install openai-whisper\n",
    "# pip install pyaudio  # Required for microphone input with speech_recognition\n",
    "# Initialize recognizer and Whisper model\n",
    "import speech_recognition as sr\n",
    "import whisper\n",
    "recognizer = sr.Recognizer()\n",
    "model = whisper.load_model(\"base\")  # or \"tiny\", \"small\", \"medium\", \"large\"\n",
    "# Use microphone as source\n",
    "with sr.Microphone() as source:\n",
    "    print(\"Adjusting for ambient noise... Please wait...\")\n",
    "    recognizer.adjust_for_ambient_noise(source, duration = 1)\n",
    "    \n",
    "    print(\"\\nListening... Speak now!\")\n",
    "    audio = recognizer.listen(source)\n",
    "    \n",
    "    print(\"Processing speech with Whisper...\")\n",
    "    try:\n",
    "        # Convert speech to text using Whisper\n",
    "        text = recognizer.recognize_whisper(audio, model = \"base\")\n",
    "        print(\"You said:\", text)\n",
    "    except sr.UnknownValueError:\n",
    "        print(\"Whisper could not understand audio\")\n",
    "    except Exception as e:\n",
    "        print(\"Error during recognition:\", str(e))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How Gemini GenerativeModel works?\n",
    "\n",
    "The `genai.GenerativeModel` class in the `google.generativeai` library wraps default parameters for calls to `generate_content`, `count_tokens`, and `start_chat`. This class supports multi-turn conversations and multimodal requests. The supported media types for input and output depend on the model.\n",
    "\n",
    "#### Usage\n",
    "\n",
    "#### Importing and Configuration\n",
    "First, import the necessary libraries and configure the API key:\n",
    "```python\n",
    "import google.generativeai as genai\n",
    "import PIL.Image\n",
    "genai.configure(api_key='YOUR_API_KEY')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;31mType:\u001b[0m        GenerativeModel\n",
      "\u001b[1;31mString form:\u001b[0m\n",
      "genai.GenerativeModel(\n",
      "    model_name='models/gemini-1.5-flash-8b',\n",
      "    generation_config={},\n",
      "    safety_settings={},\n",
      "    tools=None,\n",
      "    system_instruction=None,\n",
      "    cached_content=None\n",
      ")\n",
      "\u001b[1;31mFile:\u001b[0m        c:\\users\\anita\\desktop\\godental_ai\\myenv\\lib\\site-packages\\google\\generativeai\\generative_models.py\n",
      "\u001b[1;31mDocstring:\u001b[0m  \n",
      "The `genai.GenerativeModel` class wraps default parameters for calls to\n",
      "`GenerativeModel.generate_content`, `GenerativeModel.count_tokens`, and\n",
      "`GenerativeModel.start_chat`.\n",
      "\n",
      "This family of functionality is designed to support multi-turn conversations, and multimodal\n",
      "requests. What media-types are supported for input and output is model-dependant.\n",
      "\n",
      ">>> import google.generativeai as genai\n",
      ">>> import PIL.Image\n",
      ">>> genai.configure(api_key='YOUR_API_KEY')\n",
      ">>> model = genai.GenerativeModel('models/gemini-pro')\n",
      ">>> result = model.generate_content('Tell me a story about a magic backpack')\n",
      ">>> result.text\n",
      "\"In the quaint little town of Lakeside, there lived a young girl named Lily...\"\n",
      "\n",
      "Multimodal input:\n",
      "\n",
      ">>> model = genai.GenerativeModel('models/gemini-pro')\n",
      ">>> result = model.generate_content([\n",
      "...     \"Give me a recipe for these:\", PIL.Image.open('scones.jpeg')])\n",
      ">>> result.text\n",
      "\"**Blueberry Scones** ...\"\n",
      "\n",
      "Multi-turn conversation:\n",
      "\n",
      ">>> chat = model.start_chat()\n",
      ">>> response = chat.send_message(\"Hi, I have some questions for you.\")\n",
      ">>> response.text\n",
      "\"Sure, I'll do my best to answer your questions...\"\n",
      "\n",
      "To list the compatible model names use:\n",
      "\n",
      ">>> for m in genai.list_models():\n",
      "...     if 'generateContent' in m.supported_generation_methods:\n",
      "...         print(m.name)\n",
      "\n",
      "Arguments:\n",
      "     model_name: The name of the model to query. To list compatible models use\n",
      "     safety_settings: Sets the default safety filters. This controls which content is blocked\n",
      "         by the api before being returned.\n",
      "     generation_config: A `genai.GenerationConfig` setting the default generation parameters to\n",
      "         use."
     ]
    }
   ],
   "source": [
    "model = genai.GenerativeModel('gemini-1.5-flash-8b')\n",
    "model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gemini Chat History Format: Accessing Response and Role\n",
    "\n",
    "```python\n",
    "chat_history = [\n",
    "   {\n",
    "       \"parts\": [{\"text\": \"You are a dental assistant AI helping with insurance verification. Be professional and concise.\"}],\n",
    "       \"role\": \"user\"\n",
    "   },\n",
    "   {\n",
    "       \"parts\": [{\"text\": \"Okay, I'm ready. Please provide the patient's name, date of birth, insurance information (policy number, group number if applicable), and the date of service.\"}],\n",
    "       \"role\": \"model\"\n",
    "   },\n",
    "   {\n",
    "       \"parts\": [{\"text\": \"Can you check insurance coverage for a patient Matthew?\"}],\n",
    "       \"role\": \"user\"\n",
    "   }\n",
    "]\n",
    "\n",
    "# To access messages:\n",
    "for message in chat_history:\n",
    "   print(f\"{message['role']}: {message['parts'][0]['text']}\\n\")\n",
    "\n",
    "# Get specific messages:\n",
    "first_user_message = chat_history[0]['parts'][0]['text']\n",
    "last_model_response = chat_history[-1]['parts'][0]['text']\n",
    "\n",
    "# Get all user messages:\n",
    "user_messages = [\n",
    "   msg['parts'][0]['text'] \n",
    "   for msg in chat_history \n",
    "   if msg['role'] == \"user\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text-to-Speech (TTS) with Coqui TTS Library\n",
    "\n",
    "Text-to-Speech (TTS) is a technology that converts text into spoken audio. The Coqui TTS library provides multiple pre-trained models for high-quality speech synthesis.\n",
    "\n",
    "### Installation\n",
    "Follow this link for [TTS installation](https://www.youtube.com/watch?v=zRaDe08cUIk&list=PL19C7uchWZerUT0qIiEv7m2zXBs5kYl1L&index=12).\n",
    "It may require some dependencies to install for Winodows operating system.\n",
    "```markdown\n",
    "pip install TTS\n",
    "```\n",
    "\n",
    "### Available Models\n",
    "```markdown\n",
    "1. Fast but lower quality\n",
    "tts = TTS(model_name=\"tts_models/en/ljspeech/fast_pitch\")\n",
    "\n",
    "2. Balanced speed/quality\n",
    "tts = TTS(model_name=\"tts_models/en/ljspeech/tacotron2-DDC_ph\")\n",
    "\n",
    "3. Best quality but slower\n",
    "tts = TTS(model_name=\"tts_models/en/ljspeech/glow-tts\")\n",
    "\n",
    "4. Support multiple languages\n",
    "tts = TTS(model_name=\"tts_models/multilingual/multi-dataset/xtts_v2\")\n",
    "```\n",
    "\n",
    "### Understanding Model Name\n",
    "```markdown\n",
    "Format: tts_models/<language>/<dataset>/<model_architecture>\n",
    "Example: tts_models/en/ljspeech/tacotron2-DDC_ph\n",
    "```\n",
    "\n",
    "Components:\n",
    "- tts_models: Type of model\n",
    "- en: Language code\n",
    "- ljspeech: Training dataset\n",
    "- tacotron2-DDC_ph: Model architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " > tts_models/en/ljspeech/tacotron2-DDC is already downloaded.\n",
      " > vocoder_models/en/ljspeech/hifigan_v2 is already downloaded.\n",
      " > Using model: Tacotron2\n",
      " > Setting up Audio Processor...\n",
      " | > sample_rate:22050\n",
      " | > resample:False\n",
      " | > num_mels:80\n",
      " | > log_func:np.log\n",
      " | > min_level_db:-100\n",
      " | > frame_shift_ms:None\n",
      " | > frame_length_ms:None\n",
      " | > ref_level_db:20\n",
      " | > fft_size:1024\n",
      " | > power:1.5\n",
      " | > preemphasis:0.0\n",
      " | > griffin_lim_iters:60\n",
      " | > signal_norm:False\n",
      " | > symmetric_norm:True\n",
      " | > mel_fmin:0\n",
      " | > mel_fmax:8000.0\n",
      " | > pitch_fmin:1.0\n",
      " | > pitch_fmax:640.0\n",
      " | > spec_gain:1.0\n",
      " | > stft_pad_mode:reflect\n",
      " | > max_norm:4.0\n",
      " | > clip_norm:True\n",
      " | > do_trim_silence:True\n",
      " | > trim_db:60\n",
      " | > do_sound_norm:False\n",
      " | > do_amp_to_db_linear:True\n",
      " | > do_amp_to_db_mel:True\n",
      " | > do_rms_norm:False\n",
      " | > db_level:None\n",
      " | > stats_path:None\n",
      " | > base:2.718281828459045\n",
      " | > hop_length:256\n",
      " | > win_length:1024\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\anita\\Desktop\\godental_ai\\myenv\\lib\\site-packages\\TTS\\utils\\io.py:54: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(f, map_location=map_location, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " > Model's reduction rate `r` is set to: 1\n",
      " > Vocoder Model: hifigan\n",
      " > Setting up Audio Processor...\n",
      " | > sample_rate:22050\n",
      " | > resample:False\n",
      " | > num_mels:80\n",
      " | > log_func:np.log\n",
      " | > min_level_db:-100\n",
      " | > frame_shift_ms:None\n",
      " | > frame_length_ms:None\n",
      " | > ref_level_db:20\n",
      " | > fft_size:1024\n",
      " | > power:1.5\n",
      " | > preemphasis:0.0\n",
      " | > griffin_lim_iters:60\n",
      " | > signal_norm:False\n",
      " | > symmetric_norm:True\n",
      " | > mel_fmin:0\n",
      " | > mel_fmax:8000.0\n",
      " | > pitch_fmin:1.0\n",
      " | > pitch_fmax:640.0\n",
      " | > spec_gain:1.0\n",
      " | > stft_pad_mode:reflect\n",
      " | > max_norm:4.0\n",
      " | > clip_norm:True\n",
      " | > do_trim_silence:False\n",
      " | > trim_db:60\n",
      " | > do_sound_norm:False\n",
      " | > do_amp_to_db_linear:True\n",
      " | > do_amp_to_db_mel:True\n",
      " | > do_rms_norm:False\n",
      " | > db_level:None\n",
      " | > stats_path:None\n",
      " | > base:2.718281828459045\n",
      " | > hop_length:256\n",
      " | > win_length:1024\n",
      " > Generator Model: hifigan_generator\n",
      " > Discriminator Model: hifigan_discriminator\n",
      "Removing weight norm...\n"
     ]
    }
   ],
   "source": [
    "# Use Hugging Face\n",
    "from TTS.api import TTS\n",
    "tts = TTS(\"tts_models/en/ljspeech/tacotron2-DDC\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Embedding and Similarity Search Using FAISS and HuggingFace\n",
    "\n",
    "### Overview\n",
    "We are creating a text embedding and similarity search system using the FAISS vector store and HuggingFace embeddings. This system converts text into numerical vectors and performs similarity searches to find the most relevant documents.\n",
    "\n",
    "### How It Works\n",
    "1. **Text Embedding**: Text is converted into numerical vectors using HuggingFace embeddings.\n",
    "2. **Vector Store**: FAISS creates a vector store from these embeddings.\n",
    "3. **Similarity Search**: Searches the vector store for the most similar vectors to a query vector.\n",
    "\n",
    "### FAISS\n",
    "FAISS (Facebook AI Similarity Search) is a library developed by Meta's Fundamental AI Research (FAIR) team for efficient similarity search and clustering of dense vectors . It supports various algorithms and can handle large datasets efficiently [FAISS](https://github.com/facebookresearch/faiss).\n",
    "\n",
    "### Semantic Search\n",
    "Semantic search is a search technique that understands the meaning and context of queries, rather than just matching keywords. [What is semantic search, and how does it work?](https://cloud.google.com/discover/what-is-semantic-search). It uses technologies like machine learning and vector search to deliver more relevant results based on the searcher's intent and context. [A Comprehensive Semantic Search Guide - Elastic](https://www.elastic.co/what-is/semantic-search).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from langchain_community.embeddings import HuggingFaceInstructEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "def faiss_score(chunked_text):\n",
    "    # Initialize the embeddings model with specific arguments for huggingface_hub 0.25.0\n",
    "    embeddings = HuggingFaceInstructEmbeddings(\n",
    "        model_name=\"hkunlp/instructor-base\",\n",
    "        model_kwargs={\"device\": \"cpu\"},\n",
    "        encode_kwargs={\"normalize_embeddings\": True},\n",
    "        cache_folder=None\n",
    "    )\n",
    "    \n",
    "    # Create the FAISS vector store\n",
    "    faiss_index = FAISS.from_texts(\n",
    "        texts = chunked_text,\n",
    "        embedding = embeddings\n",
    "    )\n",
    "    \n",
    "    return faiss_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load INSTRUCTOR_Transformer\n",
      "max_seq_length  512\n",
      "Example 1 - Simple insurance terms:\n",
      "Document 0: policy\n",
      "Document 1: claim\n",
      "Document 2: insurance\n",
      "Document 3: health insurance\n",
      "Document 4: life insurance\n",
      "load INSTRUCTOR_Transformer\n",
      "max_seq_length  512\n",
      "\n",
      "Example 2 - Insurance terms:\n",
      "Document 0: deductible\n",
      "Document 1: claim status\n",
      "Document 2: policy number\n",
      "Document 3: health insurance\n",
      "Document 4: life insurance\n",
      "\n",
      "Example 3 - Similarity search for 'insurance':\n",
      "Similar document 0: health insurance\n",
      "Similar document 1: life insurance\n",
      "load INSTRUCTOR_Transformer\n",
      "max_seq_length  512\n",
      "\n",
      "Example 4 - Searching for 'date of birth' with misheard variations:\n",
      "Match 0: date of worth\n",
      "Match 1: data box\n",
      "Match 2: polisy number\n"
     ]
    }
   ],
   "source": [
    "# Example 1: Create a simple list of insurance terms that might be misheard\n",
    "simple_texts = [\"insurance\", \"policy\", \"claim\", \"health insurance\", \"life insurance\"]\n",
    "\n",
    "# Convert the text list into FAISS vector store for similarity searching\n",
    "# This process converts each text into a numerical vector representation\n",
    "vector_store = faiss_score(simple_texts)\n",
    "\n",
    "# View all documents in the index by performing an empty search (\"\")\n",
    "# k=len(simple_texts) ensures we get back all documents\n",
    "print(\"Example 1 - Simple insurance terms:\")\n",
    "docs = vector_store.similarity_search(\"\", k = len(simple_texts))\n",
    "# Enumerate through results and print each document with its index\n",
    "for i, doc in enumerate(docs):\n",
    "   print(f\"Document {i}: {doc.page_content}\")\n",
    "\n",
    "# Example 2: Create a more realistic list of insurance terms that might be misheard\n",
    "insurance_texts = [\n",
    "   \"health insurance\",\n",
    "   \"life insurance\",\n",
    "   \"policy number\",\n",
    "   \"claim status\",\n",
    "   \"deductible\"\n",
    "]\n",
    "# Create another vector store with these insurance terms\n",
    "vector_store2 = faiss_score(insurance_texts)\n",
    "\n",
    "# View all documents in the second vector store\n",
    "print(\"\\nExample 2 - Insurance terms:\")\n",
    "docs2 = vector_store2.similarity_search(\"\", k=len(insurance_texts))\n",
    "for i, doc in enumerate(docs2):\n",
    "   print(f\"Document {i}: {doc.page_content}\")\n",
    "\n",
    "# Example 3: Demonstrate similarity search functionality\n",
    "# Search for terms similar to \"insurance\", limiting results to top 2 matches (k=2)\n",
    "print(\"\\nExample 3 - Similarity search for 'insurance':\")\n",
    "results = vector_store2.similarity_search(\"insurance\", k = 2)\n",
    "# Print the most similar terms found\n",
    "for i, doc in enumerate(results):\n",
    "   print(f\"Similar document {i}: {doc.page_content}\")\n",
    "\n",
    "# Example 4: Demonstrate with misheard variations\n",
    "misheard_insurance = [\n",
    "   \"health insurance\",\n",
    "   \"helt insurence\",  # misheard version\n",
    "   \"life insurance\", \n",
    "   \"date of worth\",  # misheard version\n",
    "   \"data box\",\n",
    "   \"polisy number\"    # misheard version\n",
    "]\n",
    "\n",
    "# Create vector store for misheard terms\n",
    "vector_store3 = faiss_score(misheard_insurance)\n",
    "\n",
    "# Search for similar phrases\n",
    "print(\"\\nExample 4 - Searching for 'date of birth' with misheard variations:\")\n",
    "results = vector_store3.similarity_search(\"date of birth\", k=3)\n",
    "for i, doc in enumerate(results):\n",
    "   print(f\"Match {i}: {doc.page_content}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load INSTRUCTOR_Transformer\n",
      "max_seq_length  512\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# Read the CSV file\n",
    "df = pd.read_csv(\"./correction_lookup.csv\")\n",
    "\n",
    "# Create FAISS index for misheard terms\n",
    "misheard_texts = df['misheard'].tolist()\n",
    "faiss_index = faiss_score(misheard_texts)\n",
    "\n",
    "# Create a function to store indices in the dataframe\n",
    "def add_faiss_indices(df, faiss_index):\n",
    "    # Get all documents with their indices using an empty search\n",
    "    docs = faiss_index.similarity_search(\"\", k = len(df))\n",
    "    \n",
    "    # Create a mapping of document content to index\n",
    "    doc_to_index = {doc.page_content: i for i, doc in enumerate(docs)}\n",
    "    \n",
    "    # Add indices to dataframe\n",
    "    df['faiss_index'] = df['misheard'].map(doc_to_index)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Update dataframe with FAISS indices\n",
    "df = add_faiss_indices(df, faiss_index)\n",
    "\n",
    "# Save updated dataframe\n",
    "df.to_csv(\"correction_lookup_with_faiss.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best match: member id (confidence: 0.99)\n",
      "\n",
      "Did you mean:\n",
      "- member id (misheard as: memory id, confidence: 0.99)\n",
      "- member id (misheard as: mem-er id, confidence: 0.91)\n",
      "- member id (misheard as: memo id, confidence: 0.88)\n",
      "- member id (misheard as: member i'd, confidence: 0.87)\n",
      "- member id (misheard as: member id, confidence: 0.86)\n"
     ]
    }
   ],
   "source": [
    "def find_similar_terms(query, faiss_index, df, k=5, threshold=0.5):\n",
    "    # Search for similar terms with scores\n",
    "    similar_docs_with_scores = faiss_index.similarity_search_with_score(query, k=k)\n",
    "    \n",
    "    # Get the corrections and confidence scores\n",
    "    results = []\n",
    "    for doc, score in similar_docs_with_scores:\n",
    "        # Convert distance score to confidence (0-1 range)\n",
    "        confidence = 1 / (1 + score)\n",
    "        \n",
    "        # Only include matches above threshold\n",
    "        if confidence >= threshold:\n",
    "            misheard = doc.page_content\n",
    "            correction = df[df['misheard'] == misheard]['correction'].iloc[0]\n",
    "            results.append({\n",
    "                'misheard': misheard,\n",
    "                'correction': correction, \n",
    "                'confidence': confidence\n",
    "            })\n",
    "    \n",
    "    # Sort results by confidence score in descending order\n",
    "    results = sorted(results, key=lambda x: x['confidence'], reverse=True)\n",
    "    \n",
    "    if results:\n",
    "        best_match = results[0]  # Get the highest confidence match\n",
    "        print(f\"\\nBest match: {best_match['correction']} (confidence: {best_match['confidence']:.2f})\")\n",
    "        \n",
    "        print(\"\\nDid you mean:\")\n",
    "        for result in results:\n",
    "            print(f\"- {result['correction']} (misheard as: {result['misheard']}, confidence: {result['confidence']:.2f})\")\n",
    "            \n",
    "        return results  # Return the full list of results\n",
    "    else:\n",
    "        print(\"No matches found above threshold\")\n",
    "        return []  # Return empty list if no matches\n",
    "\n",
    "# Example usage:\n",
    "test_query = \"memory id\"\n",
    "similar_terms = find_similar_terms(test_query, faiss_index, df, k = 5, threshold = 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing Extraction Function Using LLM\n",
    "\n",
    "The code below shows the process of testing an extraction function using a large language model (LLM) for insurance verification.\n",
    "\n",
    "## Setup and Initialization\n",
    "\n",
    "1. **Prepare the Environment:**\n",
    "   - Load necessary libraries for environment management, data handling, and API interactions.\n",
    "   - Set up environment variables and configure the Generative Model using an API key.\n",
    "\n",
    "2. **Initialize the Class:**\n",
    "   - Define a class named `InsuranceVerifier` to handle insurance verification tasks.\n",
    "   - Set up the system prompts to guide the AI assistant's role and interactions.\n",
    "\n",
    "## Extraction Functions\n",
    "\n",
    "Each function is designed to extract specific information from text using prompts sent to the LLM:\n",
    "\n",
    "1. **Extract Insurance Status:**\n",
    "   - This function identifies whether the patientâ€™s insurance status is active or inactive.\n",
    "   - The model processes the text and returns \"Active,\" \"Inactive,\" or \"None\" if unclear.\n",
    "\n",
    "2. **Extract Dates:**\n",
    "   - This function extracts dates from text and converts them into a standard format (MM/DD/YYYY).\n",
    "   - It validates the extracted date to ensure it is correctly formatted.\n",
    "\n",
    "3. **Extract Monetary Amounts:**\n",
    "   - This function identifies dollar amounts mentioned in the text.\n",
    "   - It returns the amount as a numeric value or \"None\" if no amount is found.\n",
    "\n",
    "4. **Extract Percentages:**\n",
    "   - This function extracts percentage values from the text.\n",
    "   - It ensures the extracted value is within a valid range (0-100).\n",
    "\n",
    "5. **Extract Plan Types:**\n",
    "   - This function identifies the type of insurance plan (e.g., PPO, HMO) from the text.\n",
    "   - It returns the plan type or \"None\" if the type is unclear.\n",
    "\n",
    "6. **Extract Group Numbers:**\n",
    "   - This function extracts the insurance group number from the text.\n",
    "   - It returns the group number or \"None\" if not found.\n",
    "\n",
    "7. **Extract Time Periods:**\n",
    "   - This function identifies time periods such as benefit periods or waiting periods from the text.\n",
    "   - It returns the period in a common format (e.g., \"Calendar Year,\" \"6 months\").\n",
    "\n",
    "8. **Extract Frequency Limitations:**\n",
    "   - This function extracts frequency limitations, such as how often a benefit can be used.\n",
    "   - It returns the frequency or \"None\" if not found.\n",
    "\n",
    "9. **Extract Boolean Values:**\n",
    "   - This function identifies yes/no or required/not required answers from the text.\n",
    "   - It returns `True`, `False`, or \"None\" if unclear.\n",
    "\n",
    "## Testing\n",
    "\n",
    "1. **Create an Instance:**\n",
    "   - Instantiate the `InsuranceVerifier` class with the relevant office name.\n",
    "\n",
    "2. **Use Extraction Functions:**\n",
    "   - Apply the extraction functions to process text and retrieve information.\n",
    "\n",
    "3. **Handle Extracted Information:**\n",
    "   - Display or handle the extracted information as needed for verification purposes.\n",
    "\n",
    "This setup leverages the power of LLM to improve the efficiency and accuracy of extracting insurance-related information in a professional context.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error extracting boolean: 429 Resource has been exhausted (e.g. check quota).\n",
      "{'eligibility': {'status': 'Active', 'effective_date': '03/15/2024', 'plan_type': 'DHMO', 'group_number': '345678'}, 'benefits': {'annual_maximum': 1000.0, 'remaining_maximum': 1200.0, 'deductible': 50.0, 'deductible_met': 50.0, 'benefit_period': 'Calendar Year'}, 'coverage': {'preventive': 100, 'basic': 75, 'major': 40, 'periodontics': 50, 'endodontics': 80}, 'limitations': {'waiting_period': None, 'frequency': 'Twice per year', 'missing_tooth': None, 'pre_authorization': True}}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from typing import Optional, Dict, Any\n",
    "from dataclasses import dataclass\n",
    "from datetime import datetime\n",
    "import re\n",
    "from dotenv import load_dotenv\n",
    "import google.generativeai as genai\n",
    "import random\n",
    "\n",
    "class InsuranceVerifier:\n",
    "    def __init__(self, office_name: str):\n",
    "        # Load environment variables and setup\n",
    "        load_dotenv()\n",
    "        api_key = os.getenv('GOOGLE_API_KEY')\n",
    "        self.office_name = office_name\n",
    "        \n",
    "        # Configure Gemini\n",
    "        genai.configure(api_key=api_key)\n",
    "        self.model = genai.GenerativeModel(model_name='gemini-1.5-flash-8b')\n",
    "        \n",
    "        # Initialize chat with system prompt\n",
    "        self.system_prompt = f\"\"\"You are a dental office AI assistant specializing in insurance verification calls from {office_name}.\n",
    "        Your role is to:\n",
    "        1. Verify patient insurance coverage and benefits\n",
    "        2. Gather information about deductibles, maximums, and coverage percentages\n",
    "        3. Handle insurance-related inquiries professionally and concisely\n",
    "        4. Respond in a conversational yet professional manner\n",
    "\n",
    "        Start the call with a professional introduction like: \n",
    "        \"Hi, this is [Assistant Name] calling from {office_name}. I'm calling to verify insurance benefits for our patient [Patient Name].\"\n",
    "        \"\"\"\n",
    "        \n",
    "        self.chat = self.model.start_chat(history=[])\n",
    "        self.start_conversation = self.chat.send_message(self.system_prompt).text\n",
    "        \n",
    "        # Setup extraction helpers\n",
    "        self.extract_helpers = {\n",
    "            'eligibility': {\n",
    "                'status': self.extract_status,\n",
    "                'effective_date': self.extract_date,\n",
    "                'plan_type': self.extract_plan_type,\n",
    "                'group_number': self.extract_group_number\n",
    "            },\n",
    "            'benefits': {\n",
    "                'annual_maximum': self.extract_amount,\n",
    "                'remaining_maximum': self.extract_amount,\n",
    "                'deductible': self.extract_amount,\n",
    "                'deductible_met': self.extract_amount,\n",
    "                'benefit_period': self.extract_period\n",
    "            },\n",
    "            'coverage': {\n",
    "                'preventive': self.extract_percentage,\n",
    "                'basic': self.extract_percentage,\n",
    "                'major': self.extract_percentage,\n",
    "                'periodontics': self.extract_percentage,\n",
    "                'endodontics': self.extract_percentage\n",
    "            },\n",
    "            'limitations': {\n",
    "                'waiting_period': self.extract_period,\n",
    "                'frequency': self.extract_frequency,\n",
    "                'missing_tooth': self.extract_boolean,\n",
    "                'pre_authorization': self.extract_boolean\n",
    "            }\n",
    "        }\n",
    "\n",
    "    def extract_status(self, text: str) -> Optional[str]:\n",
    "        \"\"\"Extract insurance status from text.\"\"\"\n",
    "        try:\n",
    "            prompt = f\"\"\"Extract the insurance status from the following text.\n",
    "            Return only 'Active' if the patient is eligible/active, 'Inactive' if not eligible,\n",
    "            or 'None' if unclear. No other text.\n",
    "\n",
    "            Text: {text}\n",
    "            \"\"\"\n",
    "            \n",
    "            response = self.chat.send_message(prompt)\n",
    "            result = response.text.strip()\n",
    "            \n",
    "            if result in ['Active', 'Inactive']:\n",
    "                return result\n",
    "            return None\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error extracting status: {str(e)}\")\n",
    "            return None\n",
    "\n",
    "    def extract_date(self, text: str) -> Optional[str]:\n",
    "        \"\"\"Extract date in MM/DD/YYYY format.\"\"\"\n",
    "        try:\n",
    "            prompt = f\"\"\"Extract the date from the following text and convert it to MM/DD/YYYY format.\n",
    "            If no valid date is found, respond with 'None'.\n",
    "            Only return the formatted date or 'None', no other text.\n",
    "\n",
    "            Text: {text}\n",
    "            \"\"\"\n",
    "            \n",
    "            response = self.chat.send_message(prompt)\n",
    "            result = response.text.strip()\n",
    "            \n",
    "            if result == 'None':\n",
    "                return None\n",
    "                \n",
    "            # Validate the response looks like a date\n",
    "            parts = result.split('/')\n",
    "            if len(parts) == 3 and all(p.isdigit() for p in parts):\n",
    "                return result\n",
    "            \n",
    "            return None\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error extracting date: {str(e)}\")\n",
    "            return None\n",
    "\n",
    "    def extract_amount(self, text: str) -> Optional[float]:\n",
    "        \"\"\"Extract monetary amount from text.\"\"\"\n",
    "        try:\n",
    "            prompt = f\"\"\"Extract the dollar amount from the following text.\n",
    "            Return only the number (no $ or commas) or 'None' if no amount found.\n",
    "            \n",
    "            Text: {text}\n",
    "            \"\"\"\n",
    "            \n",
    "            response = self.chat.send_message(prompt)\n",
    "            result = response.text.strip()\n",
    "            \n",
    "            if result == 'None':\n",
    "                return None\n",
    "                \n",
    "            try:\n",
    "                return float(result)\n",
    "            except ValueError:\n",
    "                return None\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error extracting amount: {str(e)}\")\n",
    "            return None\n",
    "\n",
    "    def extract_percentage(self, text: str) -> Optional[int]:\n",
    "        \"\"\"Extract percentage from text.\"\"\"\n",
    "        try:\n",
    "            prompt = f\"\"\"Extract the percentage from the following text.\n",
    "            Return only the number (no % symbol) or 'None' if no percentage found.\n",
    "            \n",
    "            Text: {text}\n",
    "            \"\"\"\n",
    "            \n",
    "            response = self.chat.send_message(prompt)\n",
    "            result = response.text.strip()\n",
    "            \n",
    "            if result == 'None':\n",
    "                return None\n",
    "                \n",
    "            try:\n",
    "                percentage = int(result)\n",
    "                if 0 <= percentage <= 100:\n",
    "                    return percentage\n",
    "                return None\n",
    "            except ValueError:\n",
    "                return None\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error extracting percentage: {str(e)}\")\n",
    "            return None\n",
    "\n",
    "    def extract_plan_type(self, text: str) -> Optional[str]:\n",
    "        \"\"\"Extract insurance plan type from text.\"\"\"\n",
    "        try:\n",
    "            prompt = f\"\"\"Extract the insurance plan type from the following text.\n",
    "            Common types include: PPO, HMO, DHMO, Indemnity, etc.\n",
    "            Return only the plan type or 'None' if unclear. No other text.\n",
    "            \n",
    "            Text: {text}\n",
    "            \"\"\"\n",
    "            \n",
    "            response = self.chat.send_message(prompt)\n",
    "            result = response.text.strip()\n",
    "            \n",
    "            if result == 'None':\n",
    "                return None\n",
    "            return result\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error extracting plan type: {str(e)}\")\n",
    "            return None\n",
    "\n",
    "    def extract_group_number(self, text: str) -> Optional[str]:\n",
    "        \"\"\"Extract group number from text.\"\"\"\n",
    "        try:\n",
    "            prompt = f\"\"\"Extract the insurance group number from the following text.\n",
    "            Return only the group number or 'None' if not found. No other text.\n",
    "            The group number might be labeled as 'Group #', 'Group Number', or similar.\n",
    "            \n",
    "            Text: {text}\n",
    "            \"\"\"\n",
    "            \n",
    "            response = self.chat.send_message(prompt)\n",
    "            result = response.text.strip()\n",
    "            \n",
    "            if result == 'None':\n",
    "                return None\n",
    "            return result\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error extracting group number: {str(e)}\")\n",
    "            return None\n",
    "\n",
    "    def extract_period(self, text: str) -> Optional[str]:\n",
    "        \"\"\"Extract benefit or waiting period from text.\"\"\"\n",
    "        try:\n",
    "            prompt = f\"\"\"Extract the time period from the following text.\n",
    "            Common formats include: 'Calendar Year', 'Contract Year', '6 months', '12 months', etc.\n",
    "            Return only the period or 'None' if not found. No other text.\n",
    "            \n",
    "            Text: {text}\n",
    "            \"\"\"\n",
    "            \n",
    "            response = self.chat.send_message(prompt)\n",
    "            result = response.text.strip()\n",
    "            \n",
    "            if result == 'None':\n",
    "                return None\n",
    "            return result\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error extracting period: {str(e)}\")\n",
    "            return None\n",
    "\n",
    "    def extract_frequency(self, text: str) -> Optional[str]:\n",
    "        \"\"\"Extract frequency limitations from text.\"\"\"\n",
    "        try:\n",
    "            prompt = f\"\"\"Extract the frequency limitation from the following text.\n",
    "            Common formats include: 'Once every 6 months', '2 per year', 'Annual', etc.\n",
    "            Return only the frequency or 'None' if not found. No other text.\n",
    "            \n",
    "            Text: {text}\n",
    "            \"\"\"\n",
    "            \n",
    "            response = self.chat.send_message(prompt)\n",
    "            result = response.text.strip()\n",
    "            \n",
    "            if result == 'None':\n",
    "                return None\n",
    "            return result\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error extracting frequency: {str(e)}\")\n",
    "            return None\n",
    "\n",
    "    def extract_boolean(self, text: str) -> Optional[bool]:\n",
    "        \"\"\"Extract yes/no answer from text.\"\"\"\n",
    "        try:\n",
    "            prompt = f\"\"\"Is the following text indicating 'yes'/'required' or 'no'/'not required'?\n",
    "            Return only 'True', 'False', or 'None' if unclear. No other text.\n",
    "            \n",
    "            Text: {text}\n",
    "            \"\"\"\n",
    "            \n",
    "            response = self.chat.send_message(prompt)\n",
    "            result = response.text.strip().lower()\n",
    "            \n",
    "            if result == 'true':\n",
    "                return True\n",
    "            elif result == 'false':\n",
    "                return False\n",
    "            return None\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error extracting boolean: {str(e)}\")\n",
    "            return None\n",
    "\n",
    "    def verify_insurance(self, insurance_qa: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        \"\"\"Process insurance QA and extract structured information.\"\"\"\n",
    "        # Initialize verification result\n",
    "        verify = {\n",
    "            \"eligibility\": {\n",
    "                \"status\": None,\n",
    "                \"effective_date\": None,\n",
    "                \"plan_type\": None,\n",
    "                \"group_number\": None\n",
    "            },\n",
    "            \"benefits\": {\n",
    "                \"annual_maximum\": None,\n",
    "                \"remaining_maximum\": None,\n",
    "                \"deductible\": None,\n",
    "                \"deductible_met\": None,\n",
    "                \"benefit_period\": None\n",
    "            },\n",
    "            \"coverage\": {\n",
    "                \"preventive\": None,\n",
    "                \"basic\": None,\n",
    "                \"major\": None,\n",
    "                \"periodontics\": None,\n",
    "                \"endodontics\": None\n",
    "            },\n",
    "            \"limitations\": {\n",
    "                \"waiting_period\": None,\n",
    "                \"frequency\": None,\n",
    "                \"missing_tooth\": None,\n",
    "                \"pre_authorization\": None\n",
    "            }\n",
    "        }\n",
    "\n",
    "        # Process each category and field\n",
    "        for category, fields in insurance_qa.items():\n",
    "            for field, qa in fields.items():\n",
    "                if category in self.extract_helpers and field in self.extract_helpers[category]:\n",
    "                    extractor = self.extract_helpers[category][field]\n",
    "                    \n",
    "                    if isinstance(qa, dict) and 'sample_answers' in qa and qa['sample_answers']:\n",
    "                        idx = random.randint(0, 3)\n",
    "                        sample_answer = qa['sample_answers'][idx]\n",
    "                        verify[category][field] = extractor(sample_answer)\n",
    "\n",
    "        return verify\n",
    "\n",
    "# Usage example:\n",
    "# Initialize the verifier\n",
    "verifier = InsuranceVerifier(office_name='Raj dental clinic')\n",
    "\n",
    "# Import your insurance_qa dictionary\n",
    "from insurance_qa_sample import insurance_qa\n",
    "\n",
    "# Process insurance verification\n",
    "result = verifier.verify_insurance(insurance_qa)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting boolean extraction tests...\n",
      "\n",
      "\n",
      "DEBUG - Boolean Extraction:\n",
      "Question: 'Is the patient eligible?'\n",
      "Response: 'Yes, the patient is eligible'\n",
      "Raw LLM response: 'True\n",
      "'\n",
      "Cleaned result: 'TRUE'\n",
      "Returning True\n",
      "\n",
      "Test case: 'Is the patient eligible?' with response: 'Yes, the patient is eligible'\n",
      "Expected: True, Result: True\n",
      "--------------------------------------------------\n",
      "\n",
      "DEBUG - Boolean Extraction:\n",
      "Question: 'Would you mind verifying insurance coverage?'\n",
      "Response: 'Sure, what would you like to verify?'\n",
      "Raw LLM response: 'True\n",
      "'\n",
      "Cleaned result: 'TRUE'\n",
      "Returning True\n",
      "\n",
      "Test case: 'Would you mind verifying insurance coverage?' with response: 'Sure, what would you like to verify?'\n",
      "Expected: True, Result: True\n",
      "--------------------------------------------------\n",
      "\n",
      "DEBUG - Boolean Extraction:\n",
      "Question: 'Is the patient eligible?'\n",
      "Response: 'No, the patient is not eligible'\n",
      "Raw LLM response: 'False\n",
      "'\n",
      "Cleaned result: 'FALSE'\n",
      "Returning False\n",
      "\n",
      "Test case: 'Is the patient eligible?' with response: 'No, the patient is not eligible'\n",
      "Expected: False, Result: False\n",
      "--------------------------------------------------\n",
      "\n",
      "DEBUG - Boolean Extraction:\n",
      "Question: 'Would you mind verifying insurance coverage?'\n",
      "Response: 'No, I don't think that's necessary.'\n",
      "Raw LLM response: 'False\n",
      "'\n",
      "Cleaned result: 'FALSE'\n",
      "Returning False\n",
      "\n",
      "Test case: 'Would you mind verifying insurance coverage?' with response: 'No, I don't think that's necessary.'\n",
      "Expected: False, Result: False\n",
      "--------------------------------------------------\n",
      "\n",
      "DEBUG - Boolean Extraction:\n",
      "Question: 'Is the coverage active?'\n",
      "Response: 'Coverage is active'\n",
      "Raw LLM response: 'True\n",
      "'\n",
      "Cleaned result: 'TRUE'\n",
      "Returning True\n",
      "\n",
      "Test case: 'Is the coverage active?' with response: 'Coverage is active'\n",
      "Expected: True, Result: True\n",
      "--------------------------------------------------\n",
      "\n",
      "DEBUG - Boolean Extraction:\n",
      "Question: 'Would you mind verifying insurance coverage?'\n",
      "Response: 'Of course, what would you like to verify?'\n",
      "Raw LLM response: 'True\n",
      "'\n",
      "Cleaned result: 'TRUE'\n",
      "Returning True\n",
      "\n",
      "Test case: 'Would you mind verifying insurance coverage?' with response: 'Of course, what would you like to verify?'\n",
      "Expected: True, Result: True\n",
      "--------------------------------------------------\n",
      "\n",
      "DEBUG - Boolean Extraction:\n",
      "Question: 'Is the patient eligible?'\n",
      "Response: 'The patient is eligible.'\n",
      "Raw LLM response: 'True\n",
      "'\n",
      "Cleaned result: 'TRUE'\n",
      "Returning True\n",
      "\n",
      "Test case: 'Is the patient eligible?' with response: 'The patient is eligible.'\n",
      "Expected: True, Result: True\n",
      "--------------------------------------------------\n",
      "\n",
      "DEBUG - Boolean Extraction:\n",
      "Question: 'Would you mind verifying insurance coverage?'\n",
      "Response: 'I'm not sure about that.'\n",
      "Raw LLM response: 'False\n",
      "'\n",
      "Cleaned result: 'FALSE'\n",
      "Returning False\n",
      "\n",
      "Test case: 'Would you mind verifying insurance coverage?' with response: 'I'm not sure about that.'\n",
      "Expected: None, Result: False\n",
      "--------------------------------------------------\n",
      "\n",
      "DEBUG - Boolean Extraction:\n",
      "Question: 'Would you mind verifying insurance coverage?'\n",
      "Response: 'Could you please repeat that?'\n",
      "Raw LLM response: 'True\n",
      "'\n",
      "Cleaned result: 'TRUE'\n",
      "Returning True\n",
      "\n",
      "Test case: 'Would you mind verifying insurance coverage?' with response: 'Could you please repeat that?'\n",
      "Expected: None, Result: True\n",
      "--------------------------------------------------\n",
      "\n",
      "DEBUG - Boolean Extraction:\n",
      "Question: 'Is the patient eligible?'\n",
      "Response: 'I'm not sure about that.'\n",
      "Raw LLM response: 'None\n",
      "'\n",
      "Cleaned result: 'NONE'\n",
      "Returning None\n",
      "\n",
      "Test case: 'Is the patient eligible?' with response: 'I'm not sure about that.'\n",
      "Expected: None, Result: None\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import google.generativeai as genai\n",
    "from typing import Optional\n",
    "\n",
    "class TestVerification:\n",
    "    def __init__(self):\n",
    "        # Initialize Gemini model\n",
    "        load_dotenv()\n",
    "        genai.configure(api_key=os.getenv('GOOGLE_API_KEY'))\n",
    "        self.model = genai.GenerativeModel('gemini-1.5-flash-8b')\n",
    "        self.chat = self.model.start_chat()\n",
    "\n",
    "    def extract_boolean(self, question: str, response: str) -> Optional[bool]:\n",
    "        \"\"\"Extract yes/no answer from the response given a specific question.\"\"\"\n",
    "        print(f\"\\nDEBUG - Boolean Extraction:\")\n",
    "        print(f\"Question: '{question}'\")\n",
    "        print(f\"Response: '{response}'\")\n",
    "\n",
    "        try:\n",
    "            prompt = f\"\"\"\n",
    "            Given this question and response, analyze the text to determine if it indicates a positive (yes) or negative (no) response.\n",
    "            Consider common positive and negative indicators:\n",
    "\n",
    "            Common positive indicators: yes, sure, affirmative, absolutely, of course, definitely, indeed, active, \n",
    "            eligible, covered, required, approved, confirmed, consent, positive, agree, proceed, what do you want, \n",
    "            what would you like, what would you like to do, what do you want to verify, what would you like to verify, \n",
    "            what can I help you with\n",
    "            Common negative indicators: no, not, never, inactive, ineligible, not covered, not required, denied, rejected, \n",
    "            negative, disagree\n",
    "\n",
    "            Note: Questions that ask for the next action or verification, such as 'Sure, what would you like to verify?' \n",
    "            or 'What do you want to do?' should be treated as positive (yes) responses.\n",
    "\n",
    "            Question: \"{question}\"\n",
    "            Response: \"{response}\"\n",
    "\n",
    "            Return only one of the following: 'True' for positive, 'False' for negative, or 'None' if unclear. No other text.\n",
    "            \"\"\"\n",
    "\n",
    "            llm_response = self.chat.send_message({\"text\": prompt})\n",
    "            result = llm_response.text.strip().upper()  # Convert to uppercase for comparison\n",
    "\n",
    "            print(f\"Raw LLM response: '{llm_response.text}'\")\n",
    "            print(f\"Cleaned result: '{result}'\")\n",
    "\n",
    "            # Check the result\n",
    "            if result == 'TRUE':\n",
    "                print(\"Returning True\")\n",
    "                return True\n",
    "            elif result == 'FALSE':\n",
    "                print(\"Returning False\")\n",
    "                return False\n",
    "            else:\n",
    "                print(\"Returning None\")\n",
    "                return None\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error in extract_boolean: {str(e)}\")\n",
    "            return None\n",
    "\n",
    "def test_extract_boolean():\n",
    "    # Create test instance\n",
    "    verification = TestVerification()\n",
    "    \n",
    "    # Test cases\n",
    "    test_cases = [\n",
    "        (\"Is the patient eligible?\", \"Yes, the patient is eligible\", True),\n",
    "        (\"Would you mind verifying insurance coverage?\", \"Sure, what would you like to verify?\", True),\n",
    "        (\"Is the patient eligible?\", \"No, the patient is not eligible\", False),\n",
    "        (\"Would you mind verifying insurance coverage?\", \"No, I don't think that's necessary.\", False),\n",
    "        (\"Is the coverage active?\", \"Coverage is active\", True),\n",
    "        (\"Would you mind verifying insurance coverage?\", \"Of course, what would you like to verify?\", True),\n",
    "        (\"Is the patient eligible?\", \"The patient is eligible.\", True),\n",
    "        (\"Would you mind verifying insurance coverage?\", \"I'm not sure about that.\", None),\n",
    "        (\"Would you mind verifying insurance coverage?\", \"Could you please repeat that?\", None),\n",
    "        (\"Is the patient eligible?\", \"I'm not sure about that.\", None)\n",
    "    ]\n",
    "    \n",
    "    print(\"\\nStarting boolean extraction tests...\\n\")\n",
    "    \n",
    "    for question, response, expected in test_cases:\n",
    "        result = verification.extract_boolean(question, response)\n",
    "        print(f\"\\nTest case: '{question}' with response: '{response}'\")\n",
    "        print(f\"Expected: {expected}, Result: {result}\")\n",
    "        print(\"-\" * 50)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test_extract_boolean()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How the Dental AI Agent Code Pieces Come Together\n",
    "\n",
    "This document explains how various code components work together in the dental AI agent to verify insurance information efficiently.\n",
    "\n",
    "The main function coordinates all parts of the system to simulate a conversation with a patient for insurance verification. It uses natural language processing, speech recognition, and text-to-speech technologies.\n",
    "\n",
    "## Main Components\n",
    "\n",
    "1. **Libraries and Utilities:**\n",
    "   - Libraries for environment management (`os`, `warnings`), data handling (`json`, `numpy`), and time management (`time`).\n",
    "   - Utilities from custom modules to handle patient data, speech formatting, verification, and correction.\n",
    "\n",
    "2. **Loading Models and Initializing Systems:**\n",
    "   - **Speech Recognition:** Uses the `speech_recognition` library to capture and process spoken input.\n",
    "   - **Whisper Model:** A pre-trained model used for transcribing speech to text.\n",
    "   - **Text-to-Speech (TTS):** Converts text responses back into speech.\n",
    "   - **Correction System:** Uses FAISS to enhance accent handling and verify spoken input.\n",
    "\n",
    "3. **Initializing the AI Assistant:**\n",
    "   - **Patient Information:** A simulated patient is generated for the demonstration.\n",
    "   - **Insurance Verification:** The system initializes to handle verification tasks for the dental office.\n",
    "   - **Conversation Flow Manager:** Manages the conversation flow, ensuring the dialogue stays on track and transitions smoothly between phases.\n",
    "\n",
    "## Workflow\n",
    "\n",
    "1. **Initial Greeting:**\n",
    "   - The assistant starts with a professional greeting, introducing the dental office and asking for help with insurance verification.\n",
    "\n",
    "2. **Main Conversation Loop:**\n",
    "   - **Listening for Speech:** The system listens for spoken input using the microphone.\n",
    "   - **Processing Speech:** Converts spoken input into text and checks if the text needs correction based on context.\n",
    "   - **Handling Corrections:** If the input needs correction, the system enhances accent handling and may seek confirmation from the user.\n",
    "   - **Responding:** The system processes the input and generates an appropriate response, which is then converted back to speech.\n",
    "\n",
    "3. **Verification Completion:**\n",
    "   - The system checks if all required verification information has been collected.\n",
    "   - Displays and saves a summary of the verification results.\n",
    "\n",
    "4. **Error Handling:**\n",
    "   - The system handles common errors, such as misunderstood input, and requests the user to repeat or rephrase.\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "The dental AI agent combines multiple technologies to create an efficient system for insurance verification. It leverages natural language processing to understand and respond to user input, ensuring accurate and professional interactions in a dental office setting.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load INSTRUCTOR_Transformer\n",
      "max_seq_length  512\n",
      "Correction system initialized successfully\n",
      " > tts_models/en/ljspeech/tacotron2-DDC is already downloaded.\n",
      " > vocoder_models/en/ljspeech/hifigan_v2 is already downloaded.\n",
      " > Using model: Tacotron2\n",
      " > Setting up Audio Processor...\n",
      " | > sample_rate:22050\n",
      " | > resample:False\n",
      " | > num_mels:80\n",
      " | > log_func:np.log\n",
      " | > min_level_db:-100\n",
      " | > frame_shift_ms:None\n",
      " | > frame_length_ms:None\n",
      " | > ref_level_db:20\n",
      " | > fft_size:1024\n",
      " | > power:1.5\n",
      " | > preemphasis:0.0\n",
      " | > griffin_lim_iters:60\n",
      " | > signal_norm:False\n",
      " | > symmetric_norm:True\n",
      " | > mel_fmin:0\n",
      " | > mel_fmax:8000.0\n",
      " | > pitch_fmin:1.0\n",
      " | > pitch_fmax:640.0\n",
      " | > spec_gain:1.0\n",
      " | > stft_pad_mode:reflect\n",
      " | > max_norm:4.0\n",
      " | > clip_norm:True\n",
      " | > do_trim_silence:True\n",
      " | > trim_db:60\n",
      " | > do_sound_norm:False\n",
      " | > do_amp_to_db_linear:True\n",
      " | > do_amp_to_db_mel:True\n",
      " | > do_rms_norm:False\n",
      " | > db_level:None\n",
      " | > stats_path:None\n",
      " | > base:2.718281828459045\n",
      " | > hop_length:256\n",
      " | > win_length:1024\n",
      " > Model's reduction rate `r` is set to: 1\n",
      " > Vocoder Model: hifigan\n",
      " > Setting up Audio Processor...\n",
      " | > sample_rate:22050\n",
      " | > resample:False\n",
      " | > num_mels:80\n",
      " | > log_func:np.log\n",
      " | > min_level_db:-100\n",
      " | > frame_shift_ms:None\n",
      " | > frame_length_ms:None\n",
      " | > ref_level_db:20\n",
      " | > fft_size:1024\n",
      " | > power:1.5\n",
      " | > preemphasis:0.0\n",
      " | > griffin_lim_iters:60\n",
      " | > signal_norm:False\n",
      " | > symmetric_norm:True\n",
      " | > mel_fmin:0\n",
      " | > mel_fmax:8000.0\n",
      " | > pitch_fmin:1.0\n",
      " | > pitch_fmax:640.0\n",
      " | > spec_gain:1.0\n",
      " | > stft_pad_mode:reflect\n",
      " | > max_norm:4.0\n",
      " | > clip_norm:True\n",
      " | > do_trim_silence:False\n",
      " | > trim_db:60\n",
      " | > do_sound_norm:False\n",
      " | > do_amp_to_db_linear:True\n",
      " | > do_amp_to_db_mel:True\n",
      " | > do_rms_norm:False\n",
      " | > db_level:None\n",
      " | > stats_path:None\n",
      " | > base:2.718281828459045\n",
      " | > hop_length:256\n",
      " | > win_length:1024\n",
      " > Generator Model: hifigan_generator\n",
      " > Discriminator Model: hifigan_discriminator\n",
      "Removing weight norm...\n",
      "\n",
      "Patient Information Available:\n",
      "Name: John Smith\n",
      "DOB: 1..29..1982\n",
      "Member ID: 438444439\n",
      "Insurance: Delta Dental\n",
      "\n",
      "Starting in Patient Information Phase\n",
      "\n",
      " > Text splitted to sentences.\n",
      "['Hi, this is an assistant from Everest Dental Clinic.', \"I'm calling about our patient John Smith.\", 'Would you mind helping me verify insurance coverage?']\n",
      " > Processing time: 11.932626247406006\n",
      " > Real-time factor: 1.0464301970859944\n",
      "\n",
      "Listening...\n",
      "Original input:  Thank you for calling. Can you please provide me a percent data work?\n",
      "\n",
      "DEBUG - Validation:\n",
      "Input text: ' Thank you for calling. Can you please provide me a percent data work?'\n",
      "Verification started: False\n",
      "Current context: patient information\n",
      "\n",
      "DEBUG - FAISS Search:\n",
      "Query: ' Thank you for calling. Can you please provide me a percent data work?'\n",
      "Matched: 'percent data buff'\n",
      "Correction: 'patient date of birth'\n",
      "Context: 'patient information'\n",
      "Current context: 'patient information'\n",
      "Confidence: 0.8473517624792021\n",
      "Input needs correction: Patient information correction found: patient date of birth\n",
      "\n",
      "DEBUG - Accent Handling:\n",
      "Original text: ' Thank you for calling. Can you please provide me a percent data work?'\n",
      "Current context: patient information\n",
      "\n",
      "DEBUG - FAISS Search:\n",
      "Query: 'thank you for calling. can you  provide me a percent data work'\n",
      "Matched: 'percent data buff'\n",
      "Correction: 'patient date of birth'\n",
      "Context: 'patient information'\n",
      "Current context: 'patient information'\n",
      "Confidence: 0.8512729771801957\n",
      "Processing request: 'thank you for calling. can you  provide me a percent data work' in context: patient information\n",
      "FAISS correction found: 'patient date of birth' with confidence: 0.8512729771801957\n",
      "Final corrected text: 'patient date of birth'\n",
      "Auto-corrected input: patient date of birth\n",
      "Processing response: patient date of birth\n",
      "\n",
      "=== DEBUG - Process Response Start ===\n",
      "Input text: 'patient date of birth'\n",
      "Current phase: Patient Info\n",
      "Current asked fields: {'dob'}\n",
      "Current provided fields: {'dob'}\n",
      "Response: The date of birth is 1..29..1982, Is transition: False\n",
      "\n",
      "Responding: The date of birth is 1..29..1982\n",
      " > Text splitted to sentences.\n",
      "['The date of birth is 1..29..1982']\n",
      " > Processing time: 6.167068004608154\n",
      " > Real-time factor: 1.2351388742698173\n",
      "\n",
      "Listening...\n",
      "Original input:  and remember I did\n",
      "\n",
      "DEBUG - Validation:\n",
      "Input text: ' and remember I did'\n",
      "Verification started: False\n",
      "Current context: patient information\n",
      "\n",
      "DEBUG - FAISS Search:\n",
      "Query: ' and remember I did'\n",
      "Matched: 'remember I did'\n",
      "Correction: 'member id'\n",
      "Context: 'patient information'\n",
      "Current context: 'patient information'\n",
      "Confidence: 0.9557227182429004\n",
      "Input needs correction: Patient information correction found: member id\n",
      "\n",
      "DEBUG - Accent Handling:\n",
      "Original text: ' and remember I did'\n",
      "Current context: patient information\n",
      "\n",
      "DEBUG - FAISS Search:\n",
      "Query: 'remember i did'\n",
      "Matched: 'remember I did'\n",
      "Correction: 'member id'\n",
      "Context: 'patient information'\n",
      "Current context: 'patient information'\n",
      "Confidence: 0.986327560575425\n",
      "Processing request: 'remember i did' in context: patient information\n",
      "FAISS correction found: 'member id' with confidence: 0.986327560575425\n",
      "Final corrected text: 'member id'\n",
      "Auto-corrected input: member id\n",
      "Processing response: member id\n",
      "\n",
      "=== DEBUG - Process Response Start ===\n",
      "Input text: 'member id'\n",
      "Current phase: Patient Info\n",
      "Current asked fields: {'member_id', 'dob'}\n",
      "Current provided fields: {'member_id', 'dob'}\n",
      "Response: The member I..D is 438444439, Is transition: False\n",
      "\n",
      "Responding: The member I..D is 4 3 8 4 4 4 4 3 9\n",
      " > Text splitted to sentences.\n",
      "['The member I.', '.D is 4 3 8 4 4 4 4 3 9']\n",
      " > Processing time: 6.988335132598877\n",
      " > Real-time factor: 1.18227342924292\n",
      "\n",
      "Listening...\n",
      "Original input:  How may I help you today?\n",
      "\n",
      "DEBUG - Validation:\n",
      "Input text: ' How may I help you today?'\n",
      "Verification started: False\n",
      "Current context: patient information\n",
      "\n",
      "DEBUG - FAISS Search:\n",
      "Query: ' How may I help you today?'\n",
      "Matched: 'howmay I help'\n",
      "Correction: 'how may I help'\n",
      "Context: 'patient information'\n",
      "Current context: 'patient information'\n",
      "Confidence: 0.8831259423919998\n",
      "Input needs correction: Patient information correction found: how may I help\n",
      "\n",
      "DEBUG - Accent Handling:\n",
      "Original text: ' How may I help you today?'\n",
      "Current context: patient information\n",
      "\n",
      "DEBUG - FAISS Search:\n",
      "Query: 'how may i help you today'\n",
      "Matched: 'how me help you'\n",
      "Correction: 'how may I help you'\n",
      "Context: 'patient information'\n",
      "Current context: 'patient information'\n",
      "Confidence: 0.8957215332265359\n",
      "Processing request: 'how may i help you today' in context: patient information\n",
      "FAISS correction found: 'how may I help you' with confidence: 0.8957215332265359\n",
      "Final corrected text: 'how may I help you'\n",
      "Auto-corrected input: how may I help you\n",
      "Processing response: how may I help you\n",
      "\n",
      "=== DEBUG - Process Response Start ===\n",
      "Input text: 'how may I help you'\n",
      "Current phase: Patient Info\n",
      "Is help phrase? True\n",
      "Help phrase detected - asking for verification\n",
      "Response: Would you mind verifying patient insurance coverage?, Is transition: False\n",
      "\n",
      "Responding: Would you mind verifying patient insurance coverage?\n",
      " > Text splitted to sentences.\n",
      "['Would you mind verifying patient insurance coverage?']\n",
      " > Processing time: 4.145056962966919\n",
      " > Real-time factor: 1.1085594075468241\n",
      "\n",
      "Listening...\n",
      "Original input:  Of course, what do you like to verify?\n",
      "\n",
      "DEBUG - Validation:\n",
      "Input text: ' Of course, what do you like to verify?'\n",
      "Verification started: False\n",
      "Current context: patient information\n",
      "\n",
      "DEBUG - FAISS Search:\n",
      "Query: ' Of course, what do you like to verify?'\n",
      "Matched: 'you like what verify'\n",
      "Correction: 'what do you like to verify'\n",
      "Context: 'patient information'\n",
      "Current context: 'patient information'\n",
      "Confidence: 0.8944745422251446\n",
      "Input needs correction: Consent response correction found: what do you like to verify\n",
      "\n",
      "DEBUG - Accent Handling:\n",
      "Original text: ' Of course, what do you like to verify?'\n",
      "Current context: patient information\n",
      "\n",
      "DEBUG - FAISS Search:\n",
      "Query: 'of course'\n",
      "Matched: 'of coarse what needed'\n",
      "Correction: 'of course what's needed'\n",
      "Context: 'patient information'\n",
      "Current context: 'patient information'\n",
      "Confidence: 0.8332266050195185\n",
      "Processing request: 'of course' in context: patient information\n",
      "FAISS correction found: 'of course what's needed' with confidence: 0.8332266050195185\n",
      "\n",
      "DEBUG - FAISS Search:\n",
      "Query: 'what do you like to verify'\n",
      "Matched: 'you like what verify'\n",
      "Correction: 'what do you like to verify'\n",
      "Context: 'patient information'\n",
      "Current context: 'patient information'\n",
      "Confidence: 0.9420368237913876\n",
      "Processing request: 'what do you like to verify' in context: patient information\n",
      "FAISS correction found: 'what do you like to verify' with confidence: 0.9420368237913876\n",
      "Final corrected text: 'of course what's needed, what do you like to verify'\n",
      "Auto-corrected input: of course what's needed, what do you like to verify\n",
      "Processing response: of course what's needed, what do you like to verify\n",
      "\n",
      "=== DEBUG - Process Response Start ===\n",
      "Input text: 'of course what's needed, what do you like to verify'\n",
      "Current phase: Patient Info\n",
      "Checking verification consent\n",
      "\n",
      "DEBUG - Boolean Extraction:\n",
      "Question: 'Would you mind verifying patient insurance coverage?'\n",
      "Response: 'of course what's needed, what do you like to verify'\n",
      "Raw LLM response: 'True\n",
      "'\n",
      "Cleaned result: 'TRUE'\n",
      "Returning True\n",
      "Consent: True\n",
      "Consent received - starting verification\n",
      "Response: What is the patient's current eligibility status?, Is transition: True\n",
      "\n",
      "Responding: What is the patient's current eligibility status?\n",
      " > Text splitted to sentences.\n",
      "[\"What is the patient's current eligibility status?\"]\n",
      " > Processing time: 4.2930591106414795\n",
      " > Real-time factor: 1.1849333240241917\n",
      "\n",
      "Listening...\n",
      "Original input:  The paste center\n",
      "\n",
      "DEBUG - Validation:\n",
      "Input text: ' The paste center'\n",
      "Verification started: True\n",
      "Current context: insurance verification\n",
      "Current field: status\n",
      "Current question: What is the patient's current eligibility status?\n",
      "Contains relevant info for status: False\n",
      "\n",
      "DEBUG - FAISS Search:\n",
      "Query: ' The paste center'\n",
      "Matched: 'Can you provide me paste and data board please?'\n",
      "Correction: 'can you provide me patient date of birth please?'\n",
      "Context: 'patient information'\n",
      "Current context: 'insurance verification'\n",
      "Confidence: 0.8266747767820215\n",
      "Skipping correction - context mismatch\n",
      "Input was valid, proceeding without correction\n",
      "Processing response:  The paste center\n",
      "\n",
      "=== DEBUG - Process Response Start ===\n",
      "Input text: ' The paste center'\n",
      "Current phase: Insurance Verification\n",
      "\n",
      "=== DEBUG - Insurance Verification Phase ===\n",
      "Current category: eligibility\n",
      "\n",
      "DEBUG - Status Extraction:\n",
      "Question: 'What is the patient's current eligibility status?'\n",
      "Response: ' The paste center'\n",
      "LLM status result: 'None'\n",
      "\n",
      "DEBUG - Date Extraction:\n",
      "Question: 'What is the effective date of coverage?'\n",
      "Response: ' The paste center'\n",
      "LLM date result: 'None'\n",
      "\n",
      "DEBUG - Plan Type Extraction:\n",
      "Question: 'What type of plan does the patient have?'\n",
      "Response: ' The paste center'\n",
      "LLM plan type result: 'None'\n",
      "=== DEBUG - Process Response End ===\n",
      "Response: I didn't quite get that. Could you rephrase?, Is transition: False\n",
      "\n",
      "Responding: I didn't quite get that. Could you rephrase?\n",
      " > Text splitted to sentences.\n",
      "[\"I didn't quite get that.\", 'Could you rephrase?']\n",
      " > Processing time: 4.097311496734619\n",
      " > Real-time factor: 0.990980590810354\n",
      "\n",
      "Listening...\n",
      "Original input:  patient is eligible and active.\n",
      "\n",
      "DEBUG - Validation:\n",
      "Input text: ' patient is eligible and active.'\n",
      "Verification started: True\n",
      "Current context: insurance verification\n",
      "Current field: status\n",
      "Current question: What is the patient's current eligibility status?\n",
      "Contains relevant info for status: True\n",
      "Input was valid, proceeding without correction\n",
      "Processing response:  patient is eligible and active.\n",
      "\n",
      "=== DEBUG - Process Response Start ===\n",
      "Input text: ' patient is eligible and active.'\n",
      "Current phase: Insurance Verification\n",
      "\n",
      "=== DEBUG - Insurance Verification Phase ===\n",
      "Current category: eligibility\n",
      "\n",
      "DEBUG - Status Extraction:\n",
      "Question: 'What is the patient's current eligibility status?'\n",
      "Response: ' patient is eligible and active.'\n",
      "LLM status result: 'Active'\n",
      "Extracted status: Active\n",
      "Response: What is the effective date of coverage?, Is transition: False\n",
      "\n",
      "Responding: What is the effective date of coverage?\n",
      " > Text splitted to sentences.\n",
      "['What is the effective date of coverage?']\n",
      " > Processing time: 0.9720501899719238\n",
      " > Real-time factor: 0.32954653580690224\n",
      "\n",
      "Listening...\n",
      "Original input:  of Coverage's January 1st, 2025.\n",
      "\n",
      "DEBUG - Validation:\n",
      "Input text: ' of Coverage's January 1st, 2025.'\n",
      "Verification started: True\n",
      "Current context: insurance verification\n",
      "Current field: effective_date\n",
      "Current question: What is the effective date of coverage?\n",
      "Contains relevant info for effective_date: True\n",
      "Input was valid, proceeding without correction\n",
      "Processing response:  of Coverage's January 1st, 2025.\n",
      "\n",
      "=== DEBUG - Process Response Start ===\n",
      "Input text: ' of Coverage's January 1st, 2025.'\n",
      "Current phase: Insurance Verification\n",
      "\n",
      "=== DEBUG - Insurance Verification Phase ===\n",
      "Current category: eligibility\n",
      "\n",
      "DEBUG - Date Extraction:\n",
      "Question: 'What is the effective date of coverage?'\n",
      "Response: ' of Coverage's January 1st, 2025.'\n",
      "LLM date result: '01/01/2025'\n",
      "Extracted effective_date: 01/01/2025\n",
      "Response: What type of plan does the patient have?, Is transition: False\n",
      "\n",
      "Responding: What type of plan does the patient have?\n",
      " > Text splitted to sentences.\n",
      "['What type of plan does the patient have?']\n",
      " > Processing time: 0.9244022369384766\n",
      " > Real-time factor: 0.3184058567310268\n",
      "\n",
      "Listening...\n",
      "Original input:  The patient has BPU plan.\n",
      "\n",
      "DEBUG - Validation:\n",
      "Input text: ' The patient has BPU plan.'\n",
      "Verification started: True\n",
      "Current context: insurance verification\n",
      "Current field: plan_type\n",
      "Current question: What type of plan does the patient have?\n",
      "Contains relevant info for plan_type: True\n",
      "Input was valid, proceeding without correction\n",
      "Processing response:  The patient has BPU plan.\n",
      "\n",
      "=== DEBUG - Process Response Start ===\n",
      "Input text: ' The patient has BPU plan.'\n",
      "Current phase: Insurance Verification\n",
      "\n",
      "=== DEBUG - Insurance Verification Phase ===\n",
      "Current category: eligibility\n",
      "\n",
      "DEBUG - Plan Type Extraction:\n",
      "Question: 'What type of plan does the patient have?'\n",
      "Response: ' The patient has BPU plan.'\n",
      "LLM plan type result: 'BPU'\n",
      "Extracted plan_type: BPU\n",
      "Response: Now for benefits. What is the annual maximum benefit?, Is transition: False\n",
      "\n",
      "Responding: Now for benefits. What is the annual maximum benefit?\n",
      " > Text splitted to sentences.\n",
      "['Now for benefits.', 'What is the annual maximum benefit?']\n",
      " > Processing time: 4.380828619003296\n",
      " > Real-time factor: 0.9067782277807024\n",
      "\n",
      "Listening...\n",
      "Original input:  The annual maximum benefit is $1,500.\n",
      "\n",
      "DEBUG - Validation:\n",
      "Input text: ' The annual maximum benefit is $1,500.'\n",
      "Verification started: True\n",
      "Current context: insurance verification\n",
      "Current field: annual_maximum\n",
      "Current question: What is the annual maximum benefit?\n",
      "Contains relevant info for annual_maximum: True\n",
      "Input was valid, proceeding without correction\n",
      "Processing response:  The annual maximum benefit is $1,500.\n",
      "\n",
      "=== DEBUG - Process Response Start ===\n",
      "Input text: ' The annual maximum benefit is $1,500.'\n",
      "Current phase: Insurance Verification\n",
      "\n",
      "=== DEBUG - Insurance Verification Phase ===\n",
      "Current category: benefits\n",
      "\n",
      "DEBUG - Amount Extraction:\n",
      "Question: 'What is the annual maximum benefit?'\n",
      "Response: ' The annual maximum benefit is $1,500.'\n",
      "LLM amount result: '1500'\n",
      "Extracted annual_maximum: 1500.0\n",
      "Response: What is the remaining benefit amount?, Is transition: False\n",
      "\n",
      "Responding: What is the remaining benefit amount?\n",
      " > Text splitted to sentences.\n",
      "['What is the remaining benefit amount?']\n",
      " > Processing time: 2.9185519218444824\n",
      " > Real-time factor: 1.0926730147492332\n",
      "\n",
      "Listening...\n",
      "Original input:  the remaining benefit amount is $250.\n",
      "\n",
      "DEBUG - Validation:\n",
      "Input text: ' the remaining benefit amount is $250.'\n",
      "Verification started: True\n",
      "Current context: insurance verification\n",
      "Current field: remaining_maximum\n",
      "Current question: What is the remaining benefit amount?\n",
      "Contains relevant info for remaining_maximum: True\n",
      "Input was valid, proceeding without correction\n",
      "Processing response:  the remaining benefit amount is $250.\n",
      "\n",
      "=== DEBUG - Process Response Start ===\n",
      "Input text: ' the remaining benefit amount is $250.'\n",
      "Current phase: Insurance Verification\n",
      "\n",
      "=== DEBUG - Insurance Verification Phase ===\n",
      "Current category: benefits\n",
      "\n",
      "DEBUG - Amount Extraction:\n",
      "Question: 'What is the remaining benefit amount?'\n",
      "Response: ' the remaining benefit amount is $250.'\n",
      "LLM amount result: '250'\n",
      "Extracted remaining_maximum: 250.0\n",
      "Response: What is the deductible amount?, Is transition: False\n",
      "\n",
      "Responding: What is the deductible amount?\n",
      " > Text splitted to sentences.\n",
      "['What is the deductible amount?']\n",
      " > Processing time: 2.4404044151306152\n",
      " > Real-time factor: 1.0300711591429952\n",
      "\n",
      "Exiting gracefully...\n",
      "\n",
      "Session ended\n"
     ]
    }
   ],
   "source": [
    "# Part 1: Imports and Helper Functions\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "import time\n",
    "import warnings\n",
    "import random\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "import nltk\n",
    "import numpy as np\n",
    "import speech_recognition as sr\n",
    "import whisper\n",
    "import torch\n",
    "from TTS.api import TTS\n",
    "import simpleaudio as sa\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "import google.generativeai as genai\n",
    "from langchain_community.embeddings import HuggingFaceInstructEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from scipy import signal\n",
    "from typing import Optional\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def format_date(date_str):\n",
    "    \"\"\"Format date for speech\"\"\"\n",
    "    date_obj = datetime.strptime(date_str, '%Y-%m-%d')\n",
    "    return f\"{date_obj.month}..{date_obj.day}..{date_obj.year}\"\n",
    "\n",
    "def fake_patient():\n",
    "    \"\"\"Generate fake patient data\"\"\"\n",
    "    first_names = ['John', 'Maria', 'James', 'Sarah', 'Michael']\n",
    "    last_names = ['Smith', 'Johnson', 'Williams', 'Brown', 'Jones']\n",
    "    \n",
    "    first_name = random.choice(first_names)\n",
    "    last_name = random.choice(last_names)\n",
    "    \n",
    "    return {\n",
    "        'first_name': first_name,\n",
    "        'last_name': last_name,\n",
    "        'date_of_birth': format_date((datetime.now() - timedelta(days=random.randint(6570, 29200))).strftime('%Y-%m-%d')),\n",
    "        'member_number': str(random.randint(100000000, 999999999)),\n",
    "        'group_number': str(random.randint(1000, 9999)),\n",
    "        'insurance_provider': random.choice(['Delta Dental', 'Cigna', 'MetLife', 'Aetna', 'Guardian'])\n",
    "    }\n",
    "\n",
    "def format_speech_output(text):\n",
    "    \"\"\"Format speech output\"\"\"\n",
    "    text = text.replace(' ID ', ' I..D ').replace(' id ', ' I..D ')\n",
    "    text = re.sub(r'(\\d{1,2})/(\\d{1,2})/(\\d{4})', r'\\1..\\2..\\3', text)\n",
    "    \n",
    "    # Format member ID\n",
    "    text = re.sub(\n",
    "        r'member I..D is (\\d+)', \n",
    "        lambda m: f'member I..D is {\" \".join(list(m.group(1)))}',\n",
    "        text\n",
    "    )\n",
    "    \n",
    "    # Format group number\n",
    "    text = re.sub(\n",
    "        r'group number is (\\d+)',\n",
    "        lambda m: f'group number is {\" \".join(list(m.group(1)))}',\n",
    "        text\n",
    "    )\n",
    "    \n",
    "    # Format dollar amounts\n",
    "    text = re.sub(\n",
    "        r'\\$(\\d+(?:,\\d{3})*(?:\\.\\d{2})?)',\n",
    "        lambda m: f'$ {\" \".join(list(m.group(1).replace(\",\", \"\")))}',\n",
    "        text\n",
    "    )\n",
    "    \n",
    "    # Format percentages\n",
    "    text = re.sub(\n",
    "        r'(\\d+)%',\n",
    "        lambda m: f'{\" \".join(list(m.group(1)))} percent',\n",
    "        text\n",
    "    )\n",
    "    \n",
    "    return text\n",
    "\n",
    "def get_verification_summary(verification):\n",
    "    \"\"\"Get verification status summary\"\"\"\n",
    "    summary = {\n",
    "        'collected': {},\n",
    "        'missing': [],\n",
    "        'timestamp': datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "        'status': 'incomplete'\n",
    "    }\n",
    "    \n",
    "    for category in verification.verification_data:\n",
    "        summary['collected'][category] = {}\n",
    "        for field, value in verification.verification_data[category].items():\n",
    "            if value is not None:\n",
    "                summary['collected'][category][field] = value\n",
    "            else:\n",
    "                summary['missing'].append(f\"{category}.{field}\")\n",
    "    \n",
    "    if not summary['missing']:\n",
    "        summary['status'] = 'complete'\n",
    "    \n",
    "    return summary\n",
    "\n",
    "# Part 2: Speech Recognition and Handling Functions\n",
    "\n",
    "def initialize_correction_system():\n",
    "    \"\"\"Initialize the speech correction system using FAISS and Instruct embeddings\"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(\"./correction_lookup.csv\")\n",
    "        embeddings = HuggingFaceInstructEmbeddings(\n",
    "            model_name=\"hkunlp/instructor-base\",\n",
    "            model_kwargs={\"device\": \"cpu\"},\n",
    "            encode_kwargs={\"normalize_embeddings\": True}\n",
    "        )\n",
    "        misheard_texts = df['misheard'].tolist()\n",
    "        faiss_index = FAISS.from_texts(texts=misheard_texts, embedding=embeddings)\n",
    "        print(\"Correction system initialized successfully\")\n",
    "        return faiss_index, df\n",
    "    except Exception as e:\n",
    "        print(f\"Error initializing correction system: {str(e)}\")\n",
    "        return None, None\n",
    "\n",
    "def find_similar_terms(query, faiss_index, df, current_context=\"patient information\", threshold=0.75):\n",
    "    \"\"\"Search for similar terms using FAISS index with context\"\"\"\n",
    "    if not query or not query.strip():\n",
    "        return None, 0\n",
    "        \n",
    "    try:\n",
    "        similar_docs_with_scores = faiss_index.similarity_search_with_score(query, k=1)\n",
    "        if similar_docs_with_scores:\n",
    "            doc, score = similar_docs_with_scores[0]\n",
    "            confidence = 1 / (1 + score)\n",
    "            \n",
    "            # Get row with correction and context\n",
    "            matched_row = df[df['misheard'] == doc.page_content].iloc[0]\n",
    "            \n",
    "            print(f\"\\nDEBUG - FAISS Search:\")\n",
    "            print(f\"Query: '{query}'\")\n",
    "            print(f\"Matched: '{doc.page_content}'\")\n",
    "            print(f\"Correction: '{matched_row['correction']}'\")\n",
    "            print(f\"Context: '{matched_row['context']}'\")\n",
    "            print(f\"Current context: '{current_context}'\")\n",
    "            print(f\"Confidence: {confidence}\")\n",
    "            \n",
    "            # Only return if context matches\n",
    "            if matched_row['context'].lower() == current_context.lower():\n",
    "                return {'original': query, 'correction': matched_row['correction']}, confidence\n",
    "            else:\n",
    "                print(\"Skipping correction - context mismatch\")\n",
    "                return None, 0\n",
    "                \n",
    "    except Exception as e:\n",
    "        print(f\"Error in similarity search: {str(e)}\")\n",
    "    return None, 0\n",
    "\n",
    "def get_llm_correction(text: str, verification) -> dict:\n",
    "    \"\"\"Use LLM to suggest correction when lookup.csv doesn't have a match\"\"\"\n",
    "    try:\n",
    "        prompt = f\"\"\"In an insurance verification context, suggest the most likely correction for this input.\n",
    "        \n",
    "        Common insurance verification phrases:\n",
    "        - Questions about patient info: \"What is the patient's name?\", \"What is the date of birth?\"\n",
    "        - Insurance queries: \"What is the member ID?\", \"What's the eligibility status?\"\n",
    "        - Coverage questions: \"What's the annual maximum?\", \"What's the deductible?\"\n",
    "        \n",
    "        Input: \"{text}\"\n",
    "        \n",
    "        Return JSON format:\n",
    "        {{\"suggestion\": \"corrected phrase\",\n",
    "          \"confidence\": \"high/medium/low\"}}\n",
    "        \n",
    "        Return only valid JSON, no other text.\"\"\"\n",
    "        \n",
    "        response = verification.chat.send_message({\"text\": prompt})\n",
    "        result = json.loads(response.text.strip())\n",
    "        \n",
    "        return result\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error in LLM correction: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# def validate_input_context(text: str, verification, faiss_index=None, correction_df=None) -> dict:\n",
    "#     \"\"\"Validate if input needs correction in insurance verification context\"\"\"\n",
    "#     try:\n",
    "#         print(f\"\\nDEBUG - Validation:\")\n",
    "#         print(f\"Input text: '{text}'\")\n",
    "        \n",
    "#         # First check if the input makes sense in insurance verification context\n",
    "#         prompt = f\"\"\"Determine if this input makes sense in an insurance verification conversation context which might not be\n",
    "#         semantically correct always. The input should be processed as-is if:\n",
    "#         1. It's almost grammatically correct question or statement with some missing words in between\n",
    "#         2. It's a common response in conversation (yes, sure, of course, etc.)\n",
    "#         3. It contains insurance-related terms used correctly\n",
    "#         4. It's a reasonable response to questions about verification\n",
    "#         5. It's a complete or partial sentence that conveys meaning\n",
    "#         6. It contains any insurance-related information\n",
    "#         7. It's a common conversational response\n",
    "#         8. Even if informal, the meaning is clear\n",
    "#         9. If its question about patient info, it has to semantically make sense so use faiss correction\n",
    "        \n",
    "#         Only needs correction if:\n",
    "#         1. Contains misheard/mispronounced words (like \"data birth\" instead of \"date of birth\")\n",
    "#         2. Uses incorrect terms that change meaning (like \"legible\" instead of \"eligible\")\n",
    "#         3. Has significant grammar errors that make it unclear\n",
    "        \n",
    "#         Examples that make sense (no correction needed):\n",
    "#         - Questions: \"What would you like to verify?\", \"What do you need?\"\n",
    "#         - Responses: \"Of course\", \"Yes, I can help\", \"The patient is eligible with active status\"\n",
    "#         - Statements: \"I need to check the system\", \"Let me verify that for you\"\n",
    "#         - Help phrases: \"How may help you\", \"How can I assist\", \"What would you like to verify\"\n",
    "        \n",
    "#         Return ONLY 'True' if input makes sense, 'False' if unclear/needs correction.\n",
    "#         No other text.\n",
    "        \n",
    "#         Text: \"{text}\" \"\"\"\n",
    "        \n",
    "#         response = verification.chat.send_message({\"text\": prompt})\n",
    "#         makes_sense = response.text.strip().lower() == 'true'\n",
    "#         print(f\"Makes semantic sense: {makes_sense}\")\n",
    "        \n",
    "#         if makes_sense:\n",
    "#             return {\"needs_correction\": False, \"reason\": \"Valid in verification context\"}\n",
    "\n",
    "#         # Only try FAISS and further correction if input doesn't make sense\n",
    "#         if faiss_index is not None and correction_df is not None:\n",
    "#             correction, confidence = find_similar_terms(text, faiss_index, correction_df)\n",
    "#             print(f\"FAISS confidence: {confidence}\")\n",
    "#             if correction and confidence >= 0.8:  # Only use high confidence corrections\n",
    "#                 return {\n",
    "#                     \"needs_correction\": True,\n",
    "#                     \"reason\": f\"FAISS correction (confidence: {confidence:.2f}): {correction['correction']}\"\n",
    "#                 }\n",
    "\n",
    "#         # Default to not correcting if unsure\n",
    "#         return {\"needs_correction\": False, \"reason\": \"Proceeding without correction\"}\n",
    "\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error in validation: {str(e)}\")\n",
    "#         return {\"needs_correction\": False, \"reason\": f\"Error in validation: {str(e)}\"}\n",
    "\n",
    "def validate_input_context(text: str, verification, faiss_index=None, correction_df=None) -> dict:\n",
    "    try:\n",
    "        # Get current state\n",
    "        verification_started = getattr(verification, 'verification_started', False)\n",
    "        verification_asked = hasattr(verify_patient, 'verification_asked')\n",
    "        current_context = \"insurance verification\" if verification_started else \"patient information\"\n",
    "        \n",
    "        print(f\"\\nDEBUG - Validation:\")\n",
    "        print(f\"Input text: '{text}'\")\n",
    "        print(f\"Verification started: {verification_started}\")\n",
    "        print(f\"Current context: {current_context}\")\n",
    "\n",
    "        # Insurance verification phase\n",
    "        if verification_started:\n",
    "            current_category = getattr(verification.conversation_manager, 'current_category', None)\n",
    "            if current_category:\n",
    "                current_fields = verification.conversation_manager.insurance_qa[current_category].keys()\n",
    "                current_field = None\n",
    "                \n",
    "                for field in current_fields:\n",
    "                    if verification.verification_data[current_category][field] is None:\n",
    "                        current_field = field\n",
    "                        break\n",
    "\n",
    "                if current_field:\n",
    "                    # Get current question\n",
    "                    current_question = verification.conversation_manager.insurance_qa[current_category][current_field]['question']\n",
    "                    print(f\"Current field: {current_field}\")\n",
    "                    print(f\"Current question: {current_question}\")\n",
    "\n",
    "                    # First try to extract information\n",
    "                    info_prompt = f\"\"\"Does this response contain information relevant to: {current_question}\n",
    "                    Look for:\n",
    "                    1. Dollar amounts for payment questions\n",
    "                    2. Percentages for coverage questions\n",
    "                    3. Dates for effective date/period questions\n",
    "                    4. Status terms (active, eligible) for eligibility\n",
    "                    5. Any numbers or terms that answer the question\n",
    "                    6. Even in incomplete sentences, is the required info present?\n",
    "                    \n",
    "                    Return ONLY 'True' if extractable info exists, 'False' if not.\n",
    "                    Text: \"{text}\" \"\"\"\n",
    "                    \n",
    "                    response = verification.chat.send_message({\"text\": info_prompt})\n",
    "                    has_info = response.text.strip().lower() == 'true'\n",
    "                    print(f\"Contains relevant info for {current_field}: {has_info}\")\n",
    "                    \n",
    "                    if has_info:\n",
    "                        return {\"needs_correction\": False, \"reason\": \"Contains relevant information\"}\n",
    "\n",
    "                    # If no info found, try FAISS correction\n",
    "                    if faiss_index is not None and correction_df is not None:\n",
    "                        correction, confidence = find_similar_terms(text, faiss_index, correction_df, current_context)\n",
    "                        if correction and confidence >= 0.8:\n",
    "                            return {\n",
    "                                \"needs_correction\": True,\n",
    "                                \"reason\": f\"Insurance verification correction found: {correction['correction']}\"\n",
    "                            }\n",
    "                    return {\"needs_correction\": False, \"reason\": \"Ask to rephrase\"}\n",
    "\n",
    "            return {\"needs_correction\": False, \"reason\": \"Process as conversation response\"}\n",
    "\n",
    "        # Patient information phase\n",
    "        else:\n",
    "            if faiss_index is not None and correction_df is not None:\n",
    "                correction, confidence = find_similar_terms(text, faiss_index, correction_df, current_context)\n",
    "                if correction and confidence >= 0.8:\n",
    "                    if verification_asked:\n",
    "                        return {\n",
    "                            \"needs_correction\": True,\n",
    "                            \"reason\": f\"Consent response correction found: {correction['correction']}\"\n",
    "                        }\n",
    "                    else:\n",
    "                        return {\n",
    "                            \"needs_correction\": True,\n",
    "                            \"reason\": f\"Patient information correction found: {correction['correction']}\"\n",
    "                        }\n",
    "            return {\"needs_correction\": False, \"reason\": \"No correction needed\"}\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error in validation: {str(e)}\")\n",
    "        return {\"needs_correction\": False, \"reason\": f\"Error in validation: {str(e)}\"}\n",
    "    \n",
    "# def enhance_accent_handling(text, faiss_index, correction_df, verification):\n",
    "#     \"\"\"Enhanced accent handling using FAISS for corrections with LLM fallback\"\"\"\n",
    "#     if not text or not text.strip():\n",
    "#         return {'original': text, 'corrected': text, 'needs_confirmation': False}\n",
    "        \n",
    "#     cleaned_text = text.lower().replace('?', ',').replace(' and ', ', ').replace('please', '')\n",
    "#     requests = [req.strip() for req in cleaned_text.split(',') if req.strip()]\n",
    "    \n",
    "#     corrections_made = []\n",
    "#     needs_confirmation = False\n",
    "#     corrected_requests = []\n",
    "    \n",
    "#     for request in requests:\n",
    "#         # First try FAISS lookup\n",
    "#         correction, confidence = find_similar_terms(request, faiss_index, correction_df)\n",
    "        \n",
    "#         if correction:\n",
    "#             corrected_requests.append(correction['correction'])\n",
    "#             corrections_made.append((request, correction['correction'], confidence))\n",
    "#             needs_confirmation = needs_confirmation or confidence < 0.75\n",
    "#         else:\n",
    "#             # If no FAISS match, try LLM suggestion\n",
    "#             correction = get_llm_correction(request, verification)\n",
    "#             if correction:\n",
    "#                 corrected_requests.append(correction['suggestion'])\n",
    "#                 # Map confidence levels to numerical values\n",
    "#                 confidence_map = {'high': 0.8, 'medium': 0.5, 'low': 0.3}\n",
    "#                 confidence = confidence_map.get(correction['confidence'], 0.5)\n",
    "#                 corrections_made.append((request, correction['suggestion'], confidence))\n",
    "#                 needs_confirmation = True  # Always confirm LLM suggestions\n",
    "#             else:\n",
    "#                 corrected_requests.append(request)\n",
    "                \n",
    "#     seen = set()\n",
    "#     corrected_requests = [x for x in corrected_requests if not (x in seen or seen.add(x))]\n",
    "#     corrected_text = ', '.join(corrected_requests)\n",
    "    \n",
    "#     confirmation_msg = None\n",
    "#     if needs_confirmation:\n",
    "#         confirmation_msg = \"I heard: \" + \", \".join(\n",
    "#             f\"'{orig}' (did you mean '{corr}'?)\" \n",
    "#             for orig, corr, conf in corrections_made\n",
    "#         )\n",
    "        \n",
    "#     return {\n",
    "#         'original': text,\n",
    "#         'corrected': corrected_text,\n",
    "#         'needs_confirmation': needs_confirmation,\n",
    "#         'confirmation_msg': confirmation_msg,\n",
    "#         'corrections': corrections_made\n",
    "#     }\n",
    "\n",
    "def enhance_accent_handling(text, faiss_index, correction_df, verification):\n",
    "    \"\"\"Enhanced accent handling using FAISS corrections\"\"\"\n",
    "    if not text or not text.strip():\n",
    "        return {'original': text, 'corrected': text, 'needs_confirmation': False}\n",
    "    \n",
    "    # Determine current context based on verification state\n",
    "    verification_started = getattr(verification, 'verification_started', False)\n",
    "    has_current_category = hasattr(verification, 'current_category')\n",
    "    in_verification = verification_started and has_current_category\n",
    "    current_context = \"insurance verification\" if in_verification else \"patient information\"\n",
    "    \n",
    "    print(f\"\\nDEBUG - Accent Handling:\")\n",
    "    print(f\"Original text: '{text}'\")\n",
    "    print(f\"Current context: {current_context}\")\n",
    "        \n",
    "    cleaned_text = text.lower().replace('?', ',').replace(' and ', ', ').replace('please', '')\n",
    "    requests = [req.strip() for req in cleaned_text.split(',') if req.strip()]\n",
    "    \n",
    "    corrections_made = []\n",
    "    needs_confirmation = False\n",
    "    corrected_requests = []\n",
    "    \n",
    "    for request in requests:\n",
    "        # Try FAISS lookup with context\n",
    "        correction, confidence = find_similar_terms(request, faiss_index, correction_df, current_context)\n",
    "        print(f\"Processing request: '{request}' in context: {current_context}\")\n",
    "        \n",
    "        if correction:\n",
    "            corrected_requests.append(correction['correction'])\n",
    "            corrections_made.append((request, correction['correction'], confidence))\n",
    "            needs_confirmation = needs_confirmation or confidence < 0.75\n",
    "            print(f\"FAISS correction found: '{correction['correction']}' with confidence: {confidence}\")\n",
    "        else:\n",
    "            # If no FAISS match, keep original\n",
    "            corrected_requests.append(request)\n",
    "            print(f\"No correction found for: '{request}'\")\n",
    "                \n",
    "    seen = set()\n",
    "    corrected_requests = [x for x in corrected_requests if not (x in seen or seen.add(x))]\n",
    "    corrected_text = ', '.join(corrected_requests)\n",
    "    \n",
    "    confirmation_msg = None\n",
    "    if needs_confirmation:\n",
    "        confirmation_msg = \"I heard: \" + \", \".join(\n",
    "            f\"'{orig}' (did you mean '{corr}'?)\" \n",
    "            for orig, corr, conf in corrections_made\n",
    "        )\n",
    "    \n",
    "    print(f\"Final corrected text: '{corrected_text}'\")\n",
    "    return {\n",
    "        'original': text,\n",
    "        'corrected': corrected_text,\n",
    "        'needs_confirmation': needs_confirmation,\n",
    "        'confirmation_msg': confirmation_msg,\n",
    "        'corrections': corrections_made\n",
    "    }\n",
    "\n",
    "def initialize_enhanced_recognition():\n",
    "    \"\"\"Initialize speech recognition with enhanced settings\"\"\"\n",
    "    recognizer = sr.Recognizer()\n",
    "    recognizer.energy_threshold = 3000\n",
    "    recognizer.dynamic_energy_threshold = True\n",
    "    recognizer.dynamic_energy_adjustment_damping = 0.15\n",
    "    recognizer.dynamic_energy_ratio = 1.5\n",
    "    recognizer.pause_threshold = 1.0\n",
    "    \n",
    "    def remove_noise(audio_data):\n",
    "        audio_array = np.frombuffer(audio_data.frame_data, dtype=np.int16)\n",
    "        nyquist = 22050 / 2\n",
    "        low = 300 / nyquist\n",
    "        high = 3000 / nyquist\n",
    "        b, a = signal.butter(4, [low, high], btype='band')\n",
    "        filtered_audio = signal.filtfilt(b, a, audio_array)\n",
    "        return filtered_audio\n",
    "    \n",
    "    recognizer.remove_noise = remove_noise\n",
    "    return recognizer\n",
    "\n",
    "def listen_for_speech(recognizer, source, play_obj=None):\n",
    "    \"\"\"Listen for speech with interruption handling\"\"\"\n",
    "    print(\"\\nListening...\")\n",
    "    if play_obj and play_obj.is_playing():\n",
    "        play_obj.stop()\n",
    "        \n",
    "    try:\n",
    "        full_audio_data = b''\n",
    "        is_speaking = False\n",
    "        \n",
    "        while True:\n",
    "            try:\n",
    "                audio = recognizer.listen(source, timeout=0.5)\n",
    "                if len(audio.get_raw_data()) > 0:\n",
    "                    is_speaking = True\n",
    "                    full_audio_data += audio.get_raw_data()\n",
    "                \n",
    "                if is_speaking:\n",
    "                    try:\n",
    "                        recognizer.listen(source, timeout=1.0)\n",
    "                        break\n",
    "                    except sr.WaitTimeoutError:\n",
    "                        continue\n",
    "            except sr.WaitTimeoutError:\n",
    "                if full_audio_data:\n",
    "                    break\n",
    "                continue\n",
    "                \n",
    "        if not full_audio_data:\n",
    "            return None\n",
    "            \n",
    "        return sr.AudioData(full_audio_data, audio.sample_rate, audio.sample_width)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error in listening: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def handle_speech_output(queue, play_obj, wav_data, recognizer, source):\n",
    "    \"\"\"Handle speech output with interruption detection\"\"\"\n",
    "    try:\n",
    "        wav_norm = wav_data * (32767 / max(0.01, np.max(np.abs(wav_data))))\n",
    "        audio_obj = sa.WaveObject(wav_norm.astype(np.int16), 1, 2, 22050)\n",
    "        \n",
    "        if not play_obj or not play_obj.is_playing():\n",
    "            play_obj = audio_obj.play()\n",
    "            \n",
    "            while play_obj.is_playing():\n",
    "                try:\n",
    "                    if recognizer.get_energy() > 3000:\n",
    "                        play_obj.stop()\n",
    "                        queue.clear()\n",
    "                        return None\n",
    "                    time.sleep(0.1)\n",
    "                except:\n",
    "                    continue\n",
    "                    \n",
    "            return play_obj\n",
    "        else:\n",
    "            queue.append(audio_obj)\n",
    "            return play_obj\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error in speech output: {str(e)}\")\n",
    "        return play_obj\n",
    "\n",
    "def handle_confirmation(text):\n",
    "    \"\"\"Handle confirmation responses\"\"\"\n",
    "    text_lower = text.lower()\n",
    "    return (\n",
    "        True if any(x in text_lower for x in ['yes', 'correct', 'right', 'yeah', 'yep']) \n",
    "        else False if any(x in text_lower for x in ['no', 'incorrect', 'wrong', 'nope']) \n",
    "        else None\n",
    "    )\n",
    "\n",
    "# Part 3: Insurance Verification Class\n",
    "\n",
    "class InsuranceVerification:\n",
    "    def __init__(self, office_name, patient_data):\n",
    "        self.office_name = office_name\n",
    "        self.patient_data = patient_data\n",
    "        self.verification_data = {\n",
    "            'eligibility': {\n",
    "                'status': None,\n",
    "                'effective_date': None,\n",
    "                'plan_type': None\n",
    "            },\n",
    "            'benefits': {\n",
    "                'annual_maximum': None,\n",
    "                'remaining_maximum': None,\n",
    "                'deductible': None,\n",
    "                'deductible_met': None,\n",
    "                'benefit_period': None\n",
    "            },\n",
    "            'coverage': {\n",
    "                'preventive': None,\n",
    "                'basic': None,\n",
    "                'major': None,\n",
    "                'periodontics': None,\n",
    "                'endodontics': None\n",
    "            },\n",
    "            'limitations': {\n",
    "                'waiting_period': None,\n",
    "                'frequency': None,\n",
    "                'missing_tooth': None,\n",
    "                'pre_authorization': None\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # Initialize extraction functions for each field\n",
    "        self.extraction_functions = {\n",
    "            'eligibility': {\n",
    "                'status': self.extract_status,\n",
    "                'effective_date': self.extract_date,\n",
    "                'plan_type': self.extract_plan_type\n",
    "            },\n",
    "            'benefits': {\n",
    "                'annual_maximum': self.extract_amount,\n",
    "                'remaining_maximum': self.extract_amount,\n",
    "                'deductible': self.extract_amount,\n",
    "                'deductible_met': self.extract_amount,\n",
    "                'benefit_period': self.extract_period\n",
    "            },\n",
    "            'coverage': {\n",
    "                'preventive': self.extract_percentage,\n",
    "                'basic': self.extract_percentage,\n",
    "                'major': self.extract_percentage,\n",
    "                'periodontics': self.extract_percentage,\n",
    "                'endodontics': self.extract_percentage\n",
    "            },\n",
    "            'limitations': {\n",
    "                'waiting_period': self.extract_period,\n",
    "                'frequency': self.extract_frequency,\n",
    "                'missing_tooth': self.extract_boolean,\n",
    "                'pre_authorization': self.extract_boolean\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # Initialize Gemini model\n",
    "        load_dotenv()\n",
    "        genai.configure(api_key=os.getenv('GOOGLE_API_KEY'))\n",
    "        self.model = genai.GenerativeModel('gemini-1.5-flash-8b')\n",
    "        self.chat = self.model.start_chat()\n",
    "\n",
    "    def extract_status(self, response: str, question: str) -> Optional[str]:\n",
    "        \"\"\"Extract insurance status from response\"\"\"\n",
    "        print(f\"\\nDEBUG - Status Extraction:\")\n",
    "        print(f\"Question: '{question}'\")\n",
    "        print(f\"Response: '{response}'\")\n",
    "        \n",
    "        try:\n",
    "            prompt = f\"\"\"Given this question and response, determine the insurance status.\n",
    "            Question: \"{question}\"\n",
    "            Response: \"{response}\"\n",
    "            \n",
    "            Return only 'Active' if patient is eligible/active/covered,\n",
    "            'Inactive' if not eligible/inactive/not covered,\n",
    "            or 'None' if unclear.\n",
    "\n",
    "            Examples:\n",
    "            Q. \"What is the patient's current eligibility status?\" A: \"The patient is currently active and eligible for benefits.\" -> \"Active\"\n",
    "            Q. \"What is the patient's current eligibility status?\" A: \"The patient's coverage is active and verified.\" -> \"Active\"\n",
    "            Q. \"What is the patient's current eligibility status?\" A: \"Yes, the patient is eligible and the coverage is in force.\" -> \"Active\"\n",
    "            Q. \"What is the patient's current eligibility status?\" A: \"The patient is eligible with active coverage status.\" -> \"Inactive\"\n",
    "            \"\"\"\n",
    "            \n",
    "            llm_response = self.chat.send_message({\"text\": prompt})\n",
    "            result = llm_response.text.strip()\n",
    "            print(f\"LLM status result: '{result}'\")\n",
    "            \n",
    "            if result in ['Active', 'Inactive']:\n",
    "                return result\n",
    "            return None\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error extracting status: {str(e)}\")\n",
    "            return None\n",
    "\n",
    "\n",
    "    def extract_date(self, response: str, question: str) -> Optional[str]:\n",
    "        \"\"\"Extract date in MM/DD/YYYY format from the response given a specific question.\"\"\"\n",
    "        print(f\"\\nDEBUG - Date Extraction:\")\n",
    "        print(f\"Question: '{question}'\")\n",
    "        print(f\"Response: '{response}'\")\n",
    "\n",
    "        current_year = datetime.now().year\n",
    "\n",
    "        try:\n",
    "            prompt = f\"\"\"\n",
    "            Given this question and response, extract the date and convert it to MM/DD/YYYY format.\n",
    "            If the response refers to 'calendar year' or 'year end', use the current year ({current_year}) to determine the date.\n",
    "\n",
    "            Question: \"{question}\"\n",
    "            Response: \"{response}\"\n",
    "            \n",
    "            Examples:\n",
    "            Q: \"What is their effective date of coverage?\" A: \"The coverage effective date is January 1st, {current_year}.\" -> \"01/01/{current_year}\"\n",
    "            Q: \"What is their effective date of coverage?\" A: \"Their coverage began on March 15th, {current_year}.\" -> \"03/15/{current_year}\"\n",
    "            Q: \"What is their effective date of coverage?\" A: \"The effective date shows as September 1st, {current_year - 1}.\" -> \"09/01/{current_year - 1}\"\n",
    "            Q: \"What is their effective date of coverage?\" A: \"Coverage has been effective since July 1st, {current_year}.\" -> \"07/01/{current_year}\"\n",
    "            Q: \"What is their effective date of coverage?\" A: \"Coverage has been effective since the start of the year.\" -> \"01/01/{current_year}\"\n",
    "            Q: \"What is their benefit period?\" A: \"The benefit period is calendar year, January through December.\" -> \"12/31/{current_year}\"\n",
    "            Q: \"What is their benefit period?\" A: \"Benefits run on a fiscal year, July through June.\" -> \"06/30/{current_year + 1}\"\n",
    "            Q: \"When did the patient's treatment start?\" A: \"The treatment started on May 5th, {current_year}.\" -> \"05/05/{current_year}\"\n",
    "            Q: \"What was the date of service?\" A: \"The date of service was August 20th, {current_year}.\" -> \"08/20/{current_year}\"\n",
    "            \"\"\"\n",
    "\n",
    "            llm_response = self.chat.send_message({\"text\": prompt})\n",
    "            result = llm_response.text.strip()\n",
    "            print(f\"LLM date result: '{result}'\")\n",
    "\n",
    "            if result == 'None':\n",
    "                return None\n",
    "\n",
    "            # Validate the response looks like a date\n",
    "            parts = result.split('/')\n",
    "            if len(parts) == 3 and all(p.isdigit() for p in parts):\n",
    "                return result\n",
    "\n",
    "            return None\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error extracting date: {str(e)}\")\n",
    "            return None\n",
    "\n",
    "\n",
    "    def extract_amount(self, response: str, question: str) -> Optional[float]:\n",
    "        \"\"\"Extract monetary amount from response\"\"\"\n",
    "        print(f\"\\nDEBUG - Amount Extraction:\")\n",
    "        print(f\"Question: '{question}'\")\n",
    "        print(f\"Response: '{response}'\")\n",
    "        \n",
    "        try:\n",
    "            prompt = f\"\"\"Given this question and response, extract the dollar amount.\n",
    "            Question: \"{question}\"\n",
    "            Response: \"{response}\"\n",
    "            \n",
    "            Return only the number (no $ or commas) or 'None' if no amount found.\n",
    "            \n",
    "            Examples:\n",
    "            Q: \"What is the deductible amount?\" A: \"fifty dollars\" -> \"50\"\n",
    "            Q: \"What is their annual maximum benefit?\" A: \"1k\" -> \"1000\"\n",
    "            Q: \"How much deductible has been met?\" A: \"They haven't met any of the deductible yet.\" -> \"0\"\n",
    "            \"\"\"\n",
    "            \n",
    "            llm_response = self.chat.send_message({\"text\": prompt})\n",
    "            result = llm_response.text.strip()\n",
    "            print(f\"LLM amount result: '{result}'\")\n",
    "            \n",
    "            if result == 'None':\n",
    "                return None\n",
    "                \n",
    "            try:\n",
    "                return float(result)\n",
    "            except ValueError:\n",
    "                return None\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error extracting amount: {str(e)}\")\n",
    "            return None\n",
    "\n",
    "    def extract_percentage(self, response: str, question: str) -> Optional[int]:\n",
    "        \"\"\"Extract percentage from the response given a specific question.\"\"\"\n",
    "        print(f\"\\nDEBUG - Percentage Extraction:\")\n",
    "        print(f\"Question: '{question}'\")\n",
    "        print(f\"Response: '{response}'\")\n",
    "\n",
    "        try:\n",
    "            prompt = f\"\"\"\n",
    "            Given this question and response, extract the percentage and return only the number (no % symbol) or 'None' if no percentage is found.\n",
    "            Interpret phrases like 'half coverage' as 50% and 'full coverage' as 100%.\n",
    "            \n",
    "            Question: \"{question}\"\n",
    "            Response: \"{response}\"\n",
    "            \n",
    "            Examples:\n",
    "            Q: \"What is the coverage percentage for preventive services?\" A: \"Preventive services are covered at 100%.\" -> \"100\"\n",
    "            Q: \"What about basic services?\" A: \"Basic services are covered at 80%.\" -> \"80\"\n",
    "            Q: \"What is the coverage for major services?\" A: \"Major services are covered at 50%.\" -> \"50\"\n",
    "            Q: \"What's the coverage for periodontal services?\" A: \"Periodontal services are covered at 80%.\" -> \"80\"\n",
    "            Q: \"What is the coverage percentage for preventive services?\" A: \"Preventive services have full coverage.\" -> \"100\"\n",
    "            Q: \"What about basic services?\" A: \"Basic services have half coverage.\" -> \"50\"\n",
    "            \"\"\"\n",
    "\n",
    "            llm_response = self.chat.send_message({\"text\": prompt})\n",
    "            result = llm_response.text.strip()\n",
    "            print(f\"LLM percentage result: '{result}'\")\n",
    "\n",
    "            if result == 'None':\n",
    "                return None\n",
    "\n",
    "            try:\n",
    "                percentage = int(result)\n",
    "                if 0 <= percentage <= 100:\n",
    "                    return percentage\n",
    "                return None\n",
    "            except ValueError:\n",
    "                return None\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error extracting percentage: {str(e)}\")\n",
    "            return None\n",
    "\n",
    "\n",
    "    def extract_plan_type(self, response: str, question: str) -> Optional[str]:\n",
    "        \"\"\"Extract insurance plan type from the response given a specific question.\"\"\"\n",
    "        print(f\"\\nDEBUG - Plan Type Extraction:\")\n",
    "        print(f\"Question: '{question}'\")\n",
    "        print(f\"Response: '{response}'\")\n",
    "\n",
    "        try:\n",
    "            prompt = f\"\"\"\n",
    "            Given this question and response, extract the insurance plan type.\n",
    "            Common types include: PPO, HMO, DHMO, Indemnity, etc.\n",
    "            Return only the plan type or 'None' if unclear. No other text.\n",
    "\n",
    "            Question: \"{question}\"\n",
    "            Response: \"{response}\"\n",
    "            \n",
    "            Examples:\n",
    "            Q: \"What type of plan do they have?\" A: \"This is a PPO plan.\" -> \"PPO\"\n",
    "            Q: \"What type of plan do they have?\" A: \"They have a DHMO plan.\" -> \"DHMO\"\n",
    "            Q: \"What type of plan do they have?\" A: \"It's an indemnity plan.\" -> \"Indemnity\"\n",
    "            Q: \"What type of plan do they have?\" A: \"The patient has a PPO Plus plan.\" -> \"PPO Plus\"\n",
    "            \"\"\"\n",
    "\n",
    "            llm_response = self.chat.send_message({\"text\": prompt})\n",
    "            result = llm_response.text.strip()\n",
    "            print(f\"LLM plan type result: '{result}'\")\n",
    "\n",
    "            if result == 'None':\n",
    "                return None\n",
    "            return result\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error extracting plan type: {str(e)}\")\n",
    "            return None\n",
    "\n",
    "\n",
    "    def extract_group_number(self, response: str, question: str) -> Optional[str]:\n",
    "        \"\"\"Extract group number from the response given a specific question.\"\"\"\n",
    "        print(f\"\\nDEBUG - Group Number Extraction:\")\n",
    "        print(f\"Question: '{question}'\")\n",
    "        print(f\"Response: '{response}'\")\n",
    "\n",
    "        try:\n",
    "            prompt = f\"\"\"\n",
    "            Given this question and response, extract the insurance group number.\n",
    "            The group number might be labeled as 'Group #', 'Group Number', or similar.\n",
    "            Return only the group number or 'None' if not found. No other text.\n",
    "\n",
    "            Question: \"{question}\"\n",
    "            Response: \"{response}\"\n",
    "            \n",
    "            Examples:\n",
    "            Q: \"Could you verify their group number?\" A: \"The group number is 123456.\" -> \"123456\"\n",
    "            Q: \"Could you verify their group number?\" A: \"I'm showing group number 789012.\" -> \"789012\"\n",
    "            Q: \"Could you verify their group number?\" A: \"Yes, the group number is 345678.\" -> \"345678\"\n",
    "            Q: \"Could you verify their group number?\" A: \"The verified group number is 901234.\" -> \"901234\"\n",
    "            \"\"\"\n",
    "\n",
    "            llm_response = self.chat.send_message({\"text\": prompt})\n",
    "            result = llm_response.text.strip()\n",
    "            print(f\"LLM group number result: '{result}'\")\n",
    "\n",
    "            if result == 'None':\n",
    "                return None\n",
    "            return result\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error extracting group number: {str(e)}\")\n",
    "            return None\n",
    "\n",
    "\n",
    "    def extract_period(self, response: str, question: str) -> Optional[str]:\n",
    "        \"\"\"Extract benefit or waiting period from the response given a specific question.\"\"\"\n",
    "        print(f\"\\nDEBUG - Period Extraction:\")\n",
    "        print(f\"Question: '{question}'\")\n",
    "        print(f\"Response: '{response}'\")\n",
    "\n",
    "        try:\n",
    "            prompt = f\"\"\"\n",
    "            Given this question and response, extract the time period.\n",
    "            Common formats include: 'Calendar Year', 'Contract Year', '6 months', '12 months', etc.\n",
    "            Return only the period or 'None' if not found. No other text.\n",
    "\n",
    "            Question: \"{question}\"\n",
    "            Response: \"{response}\"\n",
    "            \n",
    "            Examples:\n",
    "            Q: \"What is their benefit period?\" A: \"The benefit period is calendar year, January through December.\" -> \"Calendar Year\"\n",
    "            Q: \"What is their benefit period?\" A: \"Benefits run on a fiscal year, July through June.\" -> \"Fiscal Year\"\n",
    "            Q: \"What is their benefit period?\" A: \"It's a calendar year benefit period.\" -> \"Calendar Year\"\n",
    "            Q: \"What is their benefit period?\" A: \"The benefit period follows the calendar year.\" -> \"Calendar Year\"\n",
    "            \"\"\"\n",
    "\n",
    "            llm_response = self.chat.send_message({\"text\": prompt})\n",
    "            result = llm_response.text.strip()\n",
    "            print(f\"LLM period result: '{result}'\")\n",
    "\n",
    "            if result == 'None':\n",
    "                return None\n",
    "            return result\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error extracting period: {str(e)}\")\n",
    "            return None\n",
    "\n",
    "\n",
    "    def extract_frequency(self, response: str, question: str) -> Optional[dict]:\n",
    "        \"\"\"Extract frequency limitations from the response given a specific question.\"\"\"\n",
    "        print(f\"\\nDEBUG - Frequency Extraction:\")\n",
    "        print(f\"Question: '{question}'\")\n",
    "        print(f\"Response: '{response}'\")\n",
    "\n",
    "        try:\n",
    "            prompt = f\"\"\"\n",
    "            Given this question and response, extract the frequency limitation for each type of service mentioned.\n",
    "            Common formats include: 'Once every 6 months', '2 per year', 'Annual', etc.\n",
    "            Return a dictionary with the type of service as the key and the frequency as the value. If unable to extract, return the original response as the value.\n",
    "\n",
    "            Question: \"{question}\"\n",
    "            Response: \"{response}\"\n",
    "            \n",
    "            Examples:\n",
    "            Q: \"What are the frequency limitations?\" A: \"Cleanings are covered twice per calendar year.\" -> {{\"Cleanings\": \"twice per calendar year\"}}\n",
    "            Q: \"What are the frequency limitations?\" A: \"Exams and cleanings twice per year, x-rays once every 3 years.\" -> {{\"Exams\": \"twice per year\", \"Cleanings\": \"twice per year\", \"X-rays\": \"once every 3 years\"}}\n",
    "            Q: \"What are the frequency limitations?\" A: \"Two cleanings per year with 6 months separation required.\" -> {{\"Cleanings\": \"twice per year, 6 months separation\"}}\n",
    "            Q: \"What are the frequency limitations?\" A: \"Comprehensive exams once every 3 years, routine exams twice per year.\" -> {{\"Comprehensive exams\": \"once every 3 years\", \"Routine exams\": \"twice per year\"}}\n",
    "            \"\"\"\n",
    "\n",
    "            llm_response = self.chat.send_message({\"text\": prompt})\n",
    "            result = llm_response.text.strip()\n",
    "            print(f\"LLM frequency result: '{result}'\")\n",
    "\n",
    "            if result == 'None':\n",
    "                return {\"Original Response\": response}\n",
    "            \n",
    "            try:\n",
    "                # Convert the string representation of a dictionary to an actual dictionary\n",
    "                frequency_dict = eval(result)\n",
    "                return frequency_dict\n",
    "            except (SyntaxError, ValueError):\n",
    "                return {\"Original Response\": response}\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error extracting frequency: {str(e)}\")\n",
    "            return {\"Original Response\": response}\n",
    "\n",
    "    def extract_boolean(self, question: str, response: str) -> Optional[bool]:\n",
    "        \"\"\"Extract yes/no answer from the response given a specific question.\"\"\"\n",
    "        print(f\"\\nDEBUG - Boolean Extraction:\")\n",
    "        print(f\"Question: '{question}'\")\n",
    "        print(f\"Response: '{response}'\")\n",
    "\n",
    "        try:\n",
    "            prompt = f\"\"\"\n",
    "            Given this question and response, analyze the text to determine if it indicates a positive (yes) or negative (no) response.\n",
    "            Consider common positive and negative indicators:\n",
    "\n",
    "            Common positive indicators: yes, sure, affirmative, absolutely, of course, definitely, indeed, active, \n",
    "            eligible, covered, required, approved, confirmed, consent, positive, agree, proceed, what do you want, \n",
    "            what would you like, what would you like to do, what do you want to verify, what would you like to verify, \n",
    "            what can I help you with\n",
    "            Common negative indicators: no, not, never, inactive, ineligible, not covered, not required, denied, rejected, \n",
    "            negative, disagree\n",
    "\n",
    "            Note: Questions that ask for the next action or verification, such as 'Sure, what would you like to verify?' \n",
    "            or 'What do you want to do?' should be treated as positive (yes) responses.\n",
    "\n",
    "            Question: \"{question}\"\n",
    "            Response: \"{response}\"\n",
    "\n",
    "            Return only one of the following: 'True' for positive, 'False' for negative, or 'None' if unclear. No other text.\n",
    "            \"\"\"\n",
    "\n",
    "            llm_response = self.chat.send_message({\"text\": prompt})\n",
    "            result = llm_response.text.strip().upper()  # Convert to uppercase for comparison\n",
    "\n",
    "            print(f\"Raw LLM response: '{llm_response.text}'\")\n",
    "            print(f\"Cleaned result: '{result}'\")\n",
    "\n",
    "            # Check the result\n",
    "            if result == 'TRUE':\n",
    "                print(\"Returning True\")\n",
    "                return True\n",
    "            elif result == 'FALSE':\n",
    "                print(\"Returning False\")\n",
    "                return False\n",
    "            else:\n",
    "                print(\"Returning None\")\n",
    "                return None\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error in extract_boolean: {str(e)}\")\n",
    "            return None\n",
    "\n",
    "\n",
    "def verify_patient(patient_data, query):\n",
    "    if not hasattr(verify_patient, 'asked_fields'):\n",
    "        verify_patient.asked_fields = set()\n",
    "    if not hasattr(verify_patient, 'provided_fields'):\n",
    "        verify_patient.provided_fields = set()\n",
    "\n",
    "    query = query.lower().replace('?', ',').replace(' and ', ', ').replace('please', '').strip()\n",
    "    requests = [req.strip() for req in query.split(',') if req.strip()]\n",
    "    \n",
    "    formatted_responses = {\n",
    "        'name': f\"{patient_data['first_name']} {patient_data['last_name']}\",\n",
    "        'date_of_birth': patient_data['date_of_birth'],\n",
    "        'member_id': patient_data['member_number']\n",
    "    }\n",
    "\n",
    "    # First check if it's a name request\n",
    "    if any(word in query for word in ['name', 'who is', 'patient name']):\n",
    "        return f\"The patient's name is {formatted_responses['name']}\"\n",
    "        \n",
    "    response_parts = []\n",
    "    # Process only the first relevant request\n",
    "    for request in requests:\n",
    "        if not response_parts:  # Only process if we haven't added a response yet\n",
    "            if ('date' in request or 'birth' in request or 'dob' in request):\n",
    "                verify_patient.asked_fields.add('dob')\n",
    "                verify_patient.provided_fields.add('dob')\n",
    "                response_parts.append(f\"The date of birth is {formatted_responses['date_of_birth']}\")\n",
    "            elif ('member' in request or 'id' in request):\n",
    "                verify_patient.asked_fields.add('member_id')\n",
    "                verify_patient.provided_fields.add('member_id')\n",
    "                response_parts.append(f\"The member I..D is {formatted_responses['member_id']}\")\n",
    "\n",
    "    print(f\"Current asked fields: {verify_patient.asked_fields}\")\n",
    "    print(f\"Current provided fields: {verify_patient.provided_fields}\")\n",
    "    \n",
    "    # Just return the response without asking about verification\n",
    "    if len(response_parts) > 0:\n",
    "        return \" \".join(response_parts)\n",
    "    return \"What information would you like about the patient?\"\n",
    "\n",
    "class ConversationFlowManager:\n",
    "    def __init__(self, verification, patient):\n",
    "        self.verification = verification\n",
    "        self.patient = patient\n",
    "        self.is_patient_info_phase = True     # Start in patient info phase\n",
    "        self.verification_started = False      # Insurance verification not started yet\n",
    "        self.current_category = 'eligibility'  # Start with eligibility category\n",
    "        \n",
    "        # Define questions for each field\n",
    "        self.insurance_qa = {\n",
    "            'eligibility': {\n",
    "                'status': {\n",
    "                    'question': \"What is the patient's current eligibility status?\"\n",
    "                },\n",
    "                'effective_date': {\n",
    "                    'question': \"What is the effective date of coverage?\"\n",
    "                },\n",
    "                'plan_type': {\n",
    "                    'question': \"What type of plan does the patient have?\"\n",
    "                }\n",
    "            },\n",
    "            'benefits': {\n",
    "                'annual_maximum': {\n",
    "                    'question': \"What is the annual maximum benefit?\"\n",
    "                },\n",
    "                'remaining_maximum': {\n",
    "                    'question': \"What is the remaining benefit amount?\"\n",
    "                },\n",
    "                'deductible': {\n",
    "                    'question': \"What is the deductible amount?\"\n",
    "                },\n",
    "                'deductible_met': {\n",
    "                    'question': \"How much of the deductible has been met?\"\n",
    "                },\n",
    "                'benefit_period': {\n",
    "                    'question': \"What is the benefit period?\"\n",
    "                }\n",
    "            },\n",
    "            'coverage': {\n",
    "                'preventive': {\n",
    "                    'question': \"What is the coverage percentage for preventive services?\"\n",
    "                },\n",
    "                'basic': {\n",
    "                    'question': \"What is the coverage percentage for basic services?\"\n",
    "                },\n",
    "                'major': {\n",
    "                    'question': \"What is the coverage percentage for major services?\"\n",
    "                },\n",
    "                'periodontics': {\n",
    "                    'question': \"What is the coverage percentage for periodontal services?\"\n",
    "                },\n",
    "                'endodontics': {\n",
    "                    'question': \"What is the coverage percentage for endodontic services?\"\n",
    "                }\n",
    "            },\n",
    "            'limitations': {\n",
    "                'waiting_period': {\n",
    "                    'question': \"Are there any waiting periods?\"\n",
    "                },\n",
    "                'frequency': {\n",
    "                    'question': \"What are the frequency limitations?\"\n",
    "                },\n",
    "                'missing_tooth': {\n",
    "                    'question': \"Is there a missing tooth clause?\"\n",
    "                },\n",
    "                'pre_authorization': {\n",
    "                    'question': \"Are there any pre-authorization requirements?\"\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # Order of categories to verify\n",
    "        self.categories_order = ['eligibility', 'benefits', 'coverage', 'limitations']\n",
    "    \n",
    "    def check_transition_state(self, text: str) -> bool:\n",
    "        \"\"\"Use LLM to determine conversation state and readiness to transition\"\"\"\n",
    "        try:\n",
    "            # First check if we have all required fields\n",
    "            required_fields = {'dob', 'member_id'}\n",
    "            fields_collected = (hasattr(verify_patient, 'provided_fields') and \n",
    "                            verify_patient.provided_fields >= required_fields)\n",
    "            \n",
    "            if not fields_collected:\n",
    "                print(\"Cannot transition: Required fields not collected\")\n",
    "                return False\n",
    "                \n",
    "            # Check for transition phrases directly first\n",
    "            transition_phrases = [\n",
    "                \"how may i help\",\n",
    "                \"how can help\",\n",
    "                \"what can i help\",\n",
    "                \"how can i assist\",\n",
    "                \"what else\",\n",
    "                \"what's next\",\n",
    "                \"ready\",\n",
    "                \"let's proceed\",\n",
    "                \"go ahead\"\n",
    "            ]\n",
    "            text_lower = text.lower()\n",
    "            \n",
    "            # If it's a direct match with transition phrases\n",
    "            if any(phrase in text_lower for phrase in transition_phrases):\n",
    "                print(\"Transition phrase detected, proceeding to verification\")\n",
    "                return True\n",
    "                \n",
    "            # If no direct match, use LLM as backup\n",
    "            prompt = f\"\"\"You must respond with ONLY 'transition' or 'continue'.\n",
    "            \n",
    "            Determine if this response indicates readiness to proceed with insurance verification.\n",
    "            The person has already provided date of birth and member ID. Do not worry about semantic correctness\n",
    "            as long as the response makes sense.            \n",
    "            Common transition phrases include:\n",
    "            - Offering help (e.g., \"how may I help\")\n",
    "            - Asking what's next\n",
    "            - Indicating readiness to proceed\n",
    "            - Expressing availability to assist\n",
    "            \n",
    "            Input: {text}\n",
    "            \n",
    "            Respond ONLY with:\n",
    "            'transition' - if they're ready to proceed\n",
    "            'continue' - if they're not indicating readiness\"\"\"\n",
    "            \n",
    "            response = self.verification.chat.send_message({\"text\": prompt})\n",
    "            result = response.text.strip().lower()\n",
    "            result = result.replace('```', '').strip()\n",
    "            \n",
    "            print(f\"Required fields collected: {verify_patient.provided_fields}\")\n",
    "            print(f\"Transition check result: {result}\")\n",
    "            \n",
    "            should_transition = result == 'transition'\n",
    "            if should_transition:\n",
    "                print(\"LLM detected transition readiness\")\n",
    "            \n",
    "            return should_transition\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error in transition check: {str(e)}\")\n",
    "            return False\n",
    "            \n",
    "    def process_response(self, text, verification):\n",
    "        print(f\"\\n=== DEBUG - Process Response Start ===\")\n",
    "        print(f\"Input text: '{text}'\")\n",
    "        print(f\"Current phase: {'Patient Info' if self.is_patient_info_phase else 'Insurance Verification'}\")\n",
    "\n",
    "        if self.is_patient_info_phase:\n",
    "            if hasattr(verify_patient, 'verification_asked'):\n",
    "                print(\"Checking verification consent\")\n",
    "                question = \"Would you mind verifying patient insurance coverage?\"\n",
    "                consent = verification.extract_boolean(question, text)\n",
    "                print(f\"Consent: {consent}\")\n",
    "                \n",
    "                if consent:\n",
    "                    print(\"Consent received - starting verification\")\n",
    "                    delattr(verify_patient, 'verification_asked')\n",
    "                    self.is_patient_info_phase = False\n",
    "                    verification.verification_started = True\n",
    "                    verification.conversation_manager = self  # Pass self as conversation manager\n",
    "                    next_question = self.get_next_question()\n",
    "                    return next_question, True\n",
    "                elif consent is False:\n",
    "                    verification.verification_started = False\n",
    "                    return \"I understand. Please let me know when you're ready to proceed.\", False\n",
    "            \n",
    "            required_fields = {'dob', 'member_id'}\n",
    "            if (hasattr(verify_patient, 'provided_fields') and \n",
    "                verify_patient.provided_fields >= required_fields):\n",
    "\n",
    "                help_prompt = f\"\"\"Is this a phrase offering help or asking how to assist? They usually ask these after\n",
    "                providing patient name, dob and member id. The sentence does not have to be semantically correct as long\n",
    "                as we can interprete it as an offering of help.\n",
    "                Examples:\n",
    "                - \"How may I help you?\"\n",
    "                - \"What can help you with?\"\n",
    "                - \"How can I assist?\"\n",
    "                \n",
    "                Return only 'True' or 'False'.\n",
    "                Text: {text}\"\"\"\n",
    "                \n",
    "                is_help_phrase = verification.chat.send_message({\"text\": help_prompt}).text.strip().lower() == 'true'\n",
    "                print(f\"Is help phrase? {is_help_phrase}\")\n",
    "\n",
    "                if is_help_phrase:\n",
    "                    print(\"Help phrase detected - asking for verification\")\n",
    "                    setattr(verify_patient, 'verification_asked', True)\n",
    "                    return \"Would you mind verifying patient insurance coverage?\", False\n",
    "\n",
    "            patient_info_response = verify_patient(verification.patient_data, text)\n",
    "            return patient_info_response, False\n",
    "\n",
    "        # Rest of insurance verification phase stays the same\n",
    "        print(\"\\n=== DEBUG - Insurance Verification Phase ===\")\n",
    "        print(f\"Current category: {self.current_category}\")\n",
    "        current_fields = self.insurance_qa[self.current_category].keys()\n",
    "        \n",
    "        for field in current_fields:\n",
    "            if verification.verification_data[self.current_category][field] is None:\n",
    "                question = self.insurance_qa[self.current_category][field]['question']\n",
    "                extract_func = verification.extraction_functions[self.current_category][field]\n",
    "                \n",
    "                value = extract_func(text, question)\n",
    "                \n",
    "                if value is not None:\n",
    "                    verification.verification_data[self.current_category][field] = value\n",
    "                    next_question = self.get_next_question()\n",
    "                    print(f\"Extracted {field}: {value}\")\n",
    "                    if next_question:\n",
    "                        return next_question, False\n",
    "                    else:\n",
    "                        return \"Verification complete!\", False\n",
    "\n",
    "        print(\"=== DEBUG - Process Response End ===\")\n",
    "        return \"I didn't quite get that. Could you rephrase?\", False\n",
    "\n",
    "    def get_next_question(self):\n",
    "        \"\"\"Get next verification question based on current state\"\"\"\n",
    "        if not self.verification_started:\n",
    "            self.verification_started = True\n",
    "            # First question of verification\n",
    "            return self.insurance_qa['eligibility']['status']['question']\n",
    "\n",
    "        current_fields = self.insurance_qa[self.current_category].keys()\n",
    "        \n",
    "        # Look for next empty field in current category\n",
    "        for field in current_fields:\n",
    "            if self.verification.verification_data[self.current_category][field] is None:\n",
    "                return self.insurance_qa[self.current_category][field]['question']\n",
    "\n",
    "        # All fields in current category complete, move to next category\n",
    "        current_index = self.categories_order.index(self.current_category)\n",
    "        if current_index < len(self.categories_order) - 1:\n",
    "            next_category = self.categories_order[current_index + 1]\n",
    "            self.current_category = next_category\n",
    "            return (\n",
    "                self._get_category_intro() +\n",
    "                self.insurance_qa[next_category][list(self.insurance_qa[next_category].keys())[0]]['question']\n",
    "            )\n",
    "\n",
    "        return None  # All categories complete\n",
    "\n",
    "    def _get_category_intro(self):\n",
    "        \"\"\"Get introduction message for new category\"\"\"\n",
    "        intros = {\n",
    "            'eligibility': \"Let's verify eligibility. \",\n",
    "            'benefits': \"Now for benefits. \",\n",
    "            'coverage': \"Let's check coverage percentages. \",\n",
    "            'limitations': \"Finally, about limitations. \"\n",
    "        }\n",
    "        return intros.get(self.current_category, \"\")     \n",
    "\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function with improved conversation flow and speech formatting\"\"\"\n",
    "    try:\n",
    "        # Initialize basic components\n",
    "        nltk.download('punkt_tab', quiet=True)\n",
    "        office_name = \"Everest Dental Clinic\"\n",
    "\n",
    "        # Create instances\n",
    "        patient = fake_patient()\n",
    "        verification = InsuranceVerification(office_name, patient)\n",
    "        flow_manager = ConversationFlowManager(verification, patient)\n",
    "        faiss_index, correction_df = initialize_correction_system()\n",
    "        \n",
    "        if faiss_index is None or correction_df is None:\n",
    "            print(\"Could not initialize correction system\")\n",
    "            return\n",
    "\n",
    "        # Initialize speech components\n",
    "        tts = TTS(model_name=\"tts_models/en/ljspeech/tacotron2-DDC\", progress_bar=False)\n",
    "        recognizer = initialize_enhanced_recognition()\n",
    "        queue = []\n",
    "        play_obj = None\n",
    "\n",
    "        if not hasattr(whisper, 'cached_model'):\n",
    "            whisper.cached_model = whisper.load_model(\"base\")\n",
    "\n",
    "        # Display patient information\n",
    "        print(\"\\nPatient Information Available:\")\n",
    "        print(f\"Name: {patient['first_name']} {patient['last_name']}\")\n",
    "        print(f\"DOB: {patient['date_of_birth']}\")\n",
    "        print(f\"Member ID: {patient['member_number']}\")\n",
    "        print(f\"Insurance: {patient['insurance_provider']}\")\n",
    "        print(\"\\nStarting in Patient Information Phase\\n\")\n",
    "\n",
    "        # Initial greeting\n",
    "        initial_message = (\n",
    "            f\"Hi, this is an assistant from {office_name}. \"\n",
    "            f\"I'm calling about our patient {patient['first_name']} {patient['last_name']}. \"\n",
    "            f\"Would you mind helping me verify insurance coverage?\"\n",
    "        )\n",
    "        initial_message = format_speech_output(initial_message)\n",
    "        wav = np.array(tts.tts(text=initial_message))\n",
    "        play_obj = handle_speech_output(queue, play_obj, wav, recognizer, None)\n",
    "\n",
    "        # Main conversation loop\n",
    "        while True:\n",
    "            try:\n",
    "                with sr.Microphone() as source:\n",
    "                    # Handle interruptions during speech\n",
    "                    if play_obj and play_obj.is_playing():\n",
    "                        if recognizer.get_energy() > 3000:\n",
    "                            play_obj.stop()\n",
    "                            queue.clear()\n",
    "                            audio = listen_for_speech(recognizer, source, play_obj)\n",
    "                            if audio:\n",
    "                                text = recognizer.recognize_whisper(audio, model=\"base\")\n",
    "                                print(f\"Heard during speech: {text}\")\n",
    "                                continue\n",
    "\n",
    "                    # Process queued audio\n",
    "                    while len(queue) > 0:\n",
    "                        if play_obj and play_obj.is_playing():\n",
    "                            time.sleep(0.1)\n",
    "                            continue\n",
    "                        play_obj = queue.pop(0).play()\n",
    "\n",
    "                    # Listen for speech\n",
    "                    recognizer.adjust_for_ambient_noise(source, duration=1)\n",
    "                    audio = listen_for_speech(recognizer, source, play_obj)\n",
    "                    \n",
    "                    if not audio:\n",
    "                        continue\n",
    "\n",
    "                    # Process speech input\n",
    "                    text = recognizer.recognize_whisper(audio, model=\"base\")\n",
    "                    print(f\"Original input: {text}\")\n",
    "                    \n",
    "                    # Check for quit command\n",
    "                    if text.lower() == \"quit\":\n",
    "                        verification_summary = get_verification_summary(verification)\n",
    "                        print(\"\\nVerification Summary:\")\n",
    "                        for category, data in verification_summary['collected'].items():\n",
    "                            if data:\n",
    "                                print(f\"\\n{category.upper()}:\")\n",
    "                                for field, value in data.items():\n",
    "                                    print(f\"  {field}: {value}\")\n",
    "                        \n",
    "                        with open('verification_results.json', 'w') as f:\n",
    "                            json.dump(verification_summary, f, indent=2)\n",
    "                        print(\"\\nResults saved to verification_results.json\")\n",
    "                        break\n",
    "\n",
    "                    # First check if input needs correction in context\n",
    "                    validation_result = validate_input_context(text, verification, faiss_index, correction_df)\n",
    "                    \n",
    "                    if validation_result[\"needs_correction\"]:\n",
    "                        print(f\"Input needs correction: {validation_result['reason']}\")\n",
    "                        correction_result = enhance_accent_handling(text, faiss_index, correction_df, verification)\n",
    "                        \n",
    "                        if correction_result['needs_confirmation']:\n",
    "                            confirmation_msg = correction_result['confirmation_msg']\n",
    "                            formatted_confirmation = format_speech_output(confirmation_msg)\n",
    "                            print(f\"Seeking confirmation: {formatted_confirmation}\")\n",
    "                            wav = np.array(tts.tts(text=formatted_confirmation))\n",
    "                            play_obj = handle_speech_output(queue, play_obj, wav, recognizer, source)\n",
    "                            \n",
    "                            confirmation_audio = listen_for_speech(recognizer, source, play_obj)\n",
    "                            if confirmation_audio:\n",
    "                                confirmation = recognizer.recognize_whisper(confirmation_audio, model=\"base\")\n",
    "                                if handle_confirmation(confirmation):\n",
    "                                    text = correction_result['corrected']\n",
    "                                    print(f\"Confirmation received, using corrected input: {text}\")\n",
    "                                else:\n",
    "                                    print(\"Correction rejected, asking for rephrasing\")\n",
    "                                    error_msg = \"Could you please rephrase that?\"\n",
    "                                    wav = np.array(tts.tts(text=error_msg))\n",
    "                                    play_obj = handle_speech_output(queue, play_obj, wav, recognizer, source)\n",
    "                                    continue\n",
    "                        else:\n",
    "                            text = correction_result['corrected']\n",
    "                            print(f\"Auto-corrected input: {text}\")\n",
    "                    else:\n",
    "                        print(\"Input was valid, proceeding without correction\")\n",
    "\n",
    "                    # Process response and get next action\n",
    "                    print(f\"Processing response: {text}\")\n",
    "                    response, is_transition = flow_manager.process_response(text, verification)\n",
    "                    print(f\"Response: {response}, Is transition: {is_transition}\")\n",
    "                    \n",
    "                    if response:\n",
    "                        formatted_response = format_speech_output(response)\n",
    "                        print(f\"\\nResponding: {formatted_response}\")\n",
    "                        wav = np.array(tts.tts(text=formatted_response))\n",
    "                        play_obj = handle_speech_output(queue, play_obj, wav, recognizer, source)\n",
    "\n",
    "                    # Check for verification completion\n",
    "                    verification_summary = get_verification_summary(verification)\n",
    "                    if verification_summary['status'] == 'complete':\n",
    "                        print(\"\\nVerification Complete!\")\n",
    "                        print(\"\\nFinal Verification Summary:\")\n",
    "                        for category, data in verification_summary['collected'].items():\n",
    "                            if data:\n",
    "                                print(f\"\\n{category.upper()}:\")\n",
    "                                for field, value in data.items():\n",
    "                                    print(f\"  {field}: {value}\")\n",
    "                        \n",
    "                        completion_message = \"All verification information has been collected. Thank you for your help.\"\n",
    "                        wav = np.array(tts.tts(text=completion_message))\n",
    "                        play_obj = handle_speech_output(queue, play_obj, wav, recognizer, source)\n",
    "                        break\n",
    "\n",
    "            except sr.UnknownValueError:\n",
    "                print(\"Could not understand audio input\")\n",
    "                error_msg = \"I'm sorry, I couldn't understand that. Could you please repeat?\"\n",
    "                wav = np.array(tts.tts(text=error_msg))\n",
    "                play_obj = handle_speech_output(queue, play_obj, wav, recognizer, source)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error in main loop: {str(e)}\")\n",
    "                error_msg = \"I encountered an error. Could you please rephrase that?\"\n",
    "                wav = np.array(tts.tts(text=error_msg))\n",
    "                play_obj = handle_speech_output(queue, play_obj, wav, recognizer, source)\n",
    "\n",
    "    finally:\n",
    "        if play_obj and play_obj.is_playing():\n",
    "            play_obj.stop()\n",
    "        if hasattr(whisper, 'cached_model'):\n",
    "            del whisper.cached_model\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    try:\n",
    "        main()\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\nExiting gracefully...\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\nAn error occurred: {str(e)}\")\n",
    "    finally:\n",
    "        print(\"\\nSession ended\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Separating Code into Independent Components\n",
    "\n",
    "Here, we outline the steps to separate the code into independent components and compile them together as `main.py`. Follow these instructions to run the script using `python main.py` from your terminal. I have put the debugging prints as it is for alpha testing.\n",
    "\n",
    "### Steps\n",
    "\n",
    "1. **Create Independent Components:**\n",
    "   - Separate your functionalities into different modules (e.g., `utils.py`, `llm.py`, `stt.py`, `tts.py`, `verification.py`, `flow.py`).\n",
    "\n",
    "2. **Combine Components in `main.py`:**\n",
    "   - Import the necessary modules and libraries in `main.py`.\n",
    "   - Integrate the functionalities in the `main()` function to manage the overall workflow.\n",
    "\n",
    "3. **Setting Up the Virtual Environment:**\n",
    "   - If you are downloading this from GitHub, please activate the virtual environment first depending on your system.\n",
    "\n",
    "### Example Structure\n",
    "\n",
    "1. **Directory Structure:**\n",
    "   ```plaintext\n",
    "   .\n",
    "   â”œâ”€â”€ utils.py\n",
    "   â”œâ”€â”€ llm.py\n",
    "   â”œâ”€â”€ stt.py\n",
    "   â”œâ”€â”€ tts.py\n",
    "   â”œâ”€â”€ verification.py\n",
    "   â”œâ”€â”€ flow.py\n",
    "   â”œâ”€â”€ main.py\n",
    "   â”œâ”€â”€ requirements.txt\n",
    "   â””â”€â”€ myenv/  # Virtual environment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load INSTRUCTOR_Transformer\n",
      "max_seq_length  512\n",
      "Correction system initialized successfully\n",
      " > tts_models/en/ljspeech/tacotron2-DDC is already downloaded.\n",
      " > vocoder_models/en/ljspeech/hifigan_v2 is already downloaded.\n",
      " > Using model: Tacotron2\n",
      " > Setting up Audio Processor...\n",
      " | > sample_rate:22050\n",
      " | > resample:False\n",
      " | > num_mels:80\n",
      " | > log_func:np.log\n",
      " | > min_level_db:-100\n",
      " | > frame_shift_ms:None\n",
      " | > frame_length_ms:None\n",
      " | > ref_level_db:20\n",
      " | > fft_size:1024\n",
      " | > power:1.5\n",
      " | > preemphasis:0.0\n",
      " | > griffin_lim_iters:60\n",
      " | > signal_norm:False\n",
      " | > symmetric_norm:True\n",
      " | > mel_fmin:0\n",
      " | > mel_fmax:8000.0\n",
      " | > pitch_fmin:1.0\n",
      " | > pitch_fmax:640.0\n",
      " | > spec_gain:1.0\n",
      " | > stft_pad_mode:reflect\n",
      " | > max_norm:4.0\n",
      " | > clip_norm:True\n",
      " | > do_trim_silence:True\n",
      " | > trim_db:60\n",
      " | > do_sound_norm:False\n",
      " | > do_amp_to_db_linear:True\n",
      " | > do_amp_to_db_mel:True\n",
      " | > do_rms_norm:False\n",
      " | > db_level:None\n",
      " | > stats_path:None\n",
      " | > base:2.718281828459045\n",
      " | > hop_length:256\n",
      " | > win_length:1024\n",
      " > Model's reduction rate `r` is set to: 1\n",
      " > Vocoder Model: hifigan\n",
      " > Setting up Audio Processor...\n",
      " | > sample_rate:22050\n",
      " | > resample:False\n",
      " | > num_mels:80\n",
      " | > log_func:np.log\n",
      " | > min_level_db:-100\n",
      " | > frame_shift_ms:None\n",
      " | > frame_length_ms:None\n",
      " | > ref_level_db:20\n",
      " | > fft_size:1024\n",
      " | > power:1.5\n",
      " | > preemphasis:0.0\n",
      " | > griffin_lim_iters:60\n",
      " | > signal_norm:False\n",
      " | > symmetric_norm:True\n",
      " | > mel_fmin:0\n",
      " | > mel_fmax:8000.0\n",
      " | > pitch_fmin:1.0\n",
      " | > pitch_fmax:640.0\n",
      " | > spec_gain:1.0\n",
      " | > stft_pad_mode:reflect\n",
      " | > max_norm:4.0\n",
      " | > clip_norm:True\n",
      " | > do_trim_silence:False\n",
      " | > trim_db:60\n",
      " | > do_sound_norm:False\n",
      " | > do_amp_to_db_linear:True\n",
      " | > do_amp_to_db_mel:True\n",
      " | > do_rms_norm:False\n",
      " | > db_level:None\n",
      " | > stats_path:None\n",
      " | > base:2.718281828459045\n",
      " | > hop_length:256\n",
      " | > win_length:1024\n",
      " > Generator Model: hifigan_generator\n",
      " > Discriminator Model: hifigan_discriminator\n",
      "Removing weight norm...\n",
      "\n",
      "Patient Information Available:\n",
      "Name: Maria Johnson\n",
      "DOB: 5..12..1961\n",
      "Member ID: 660361384\n",
      "Insurance: Cigna\n",
      "\n",
      "Starting in Patient Information Phase\n",
      "\n",
      " > Text splitted to sentences.\n",
      "['Hi, this is an assistant from Everest Dental Clinic.', \"I'm calling about our patient Maria Johnson.\", 'Would you mind helping me verify insurance coverage?']\n",
      " > Processing time: 12.625511884689331\n",
      " > Real-time factor: 1.0905379859659972\n",
      "\n",
      "Listening...\n",
      "Original input:  Hi, thank you for calling. Can you provide me a patient data birth please?\n",
      "\n",
      "DEBUG - Validation:\n",
      "Input text: ' Hi, thank you for calling. Can you provide me a patient data birth please?'\n",
      "Verification started: False\n",
      "Current context: patient information\n",
      "\n",
      "Context Check - Looking in: patient information\n",
      "Text: 'date of birth'\n",
      "FAISS correction found: {'original': ' Hi, thank you for calling. Can you provide me a patient data birth please?', 'correction': 'date of birth'}, confidence: 0.9190682352883818\n",
      "Input needs correction: Patient information correction found: date of birth\n",
      "\n",
      "Context Check - Looking in: patient information\n",
      "Text: 'date of birth'\n",
      "Auto-corrected input: date of birth\n",
      "Processing response: date of birth\n",
      "\n",
      "=== DEBUG - Process Response Start ===\n",
      "Input text: 'date of birth'\n",
      "Current phase: Patient Info\n",
      "Response: The date of birth is 5..12..1961, Is transition: False\n",
      "\n",
      "Responding: The date of birth is 5..12..1961\n",
      " > Text splitted to sentences.\n",
      "['The date of birth is 5..12..1961']\n",
      " > Processing time: 5.866408586502075\n",
      " > Real-time factor: 1.2382668606636809\n",
      "\n",
      "Listening...\n",
      "Original input:  and member ID please.\n",
      "\n",
      "DEBUG - Validation:\n",
      "Input text: ' and member ID please.'\n",
      "Verification started: False\n",
      "Current context: patient information\n",
      "\n",
      "Context Check - Looking in: patient information\n",
      "Text: 'member id'\n",
      "FAISS correction found: {'original': ' and member ID please.', 'correction': 'member id'}, confidence: 0.8858000300894641\n",
      "Input needs correction: Patient information correction found: member id\n",
      "\n",
      "Context Check - Looking in: patient information\n",
      "Text: 'member id'\n",
      "Auto-corrected input: member id\n",
      "Processing response: member id\n",
      "\n",
      "=== DEBUG - Process Response Start ===\n",
      "Input text: 'member id'\n",
      "Current phase: Patient Info\n",
      "Response: The member I..D is 660361384, Is transition: False\n",
      "\n",
      "Responding: The member I..D is 6 6 0 3 6 1 3 8 4\n",
      " > Text splitted to sentences.\n",
      "['The member I.', '.D is 6 6 0 3 6 1 3 8 4']\n",
      " > Processing time: 6.546327352523804\n",
      " > Real-time factor: 1.001293827158365\n",
      "\n",
      "Listening...\n",
      "Original input:  How may I help you today?\n",
      "\n",
      "DEBUG - Validation:\n",
      "Input text: ' How may I help you today?'\n",
      "Verification started: False\n",
      "Current context: patient information\n",
      "\n",
      "Context Check - Looking in: patient information\n",
      "Text: 'how may I help'\n",
      "FAISS correction found: {'original': ' How may I help you today?', 'correction': 'how may I help'}, confidence: 0.8831259423919998\n",
      "Input needs correction: Patient information correction found: how may I help\n",
      "\n",
      "Context Check - Looking in: patient information\n",
      "Text: 'how may I help you'\n",
      "Auto-corrected input: how may I help you\n",
      "Processing response: how may I help you\n",
      "\n",
      "=== DEBUG - Process Response Start ===\n",
      "Input text: 'how may I help you'\n",
      "Current phase: Patient Info\n",
      "Is help phrase? True\n",
      "Help phrase detected - asking for verification\n",
      "Response: Would you mind verifying patient insurance coverage?, Is transition: False\n",
      "\n",
      "Responding: Would you mind verifying patient insurance coverage?\n",
      " > Text splitted to sentences.\n",
      "['Would you mind verifying patient insurance coverage?']\n",
      " > Processing time: 3.5835721492767334\n",
      " > Real-time factor: 0.9380076672786322\n",
      "\n",
      "Listening...\n",
      "Original input:  Of course, what do you like to verify?\n",
      "\n",
      "DEBUG - Validation:\n",
      "Input text: ' Of course, what do you like to verify?'\n",
      "Verification started: False\n",
      "Current context: patient information\n",
      "\n",
      "Context Check - Looking in: patient information\n",
      "Text: 'what do you like to verify'\n",
      "FAISS correction found: {'original': ' Of course, what do you like to verify?', 'correction': 'what do you like to verify'}, confidence: 0.8944745422251446\n",
      "Input needs correction: Consent response correction found: what do you like to verify\n",
      "\n",
      "Context Check - Looking in: patient information\n",
      "Text: 'what do you like to verify'\n",
      "Auto-corrected input: what do you like to verify\n",
      "Processing response: what do you like to verify\n",
      "\n",
      "=== DEBUG - Process Response Start ===\n",
      "Input text: 'what do you like to verify'\n",
      "Current phase: Patient Info\n",
      "Checking verification consent\n",
      "\n",
      "DEBUG - Boolean Extraction:\n",
      "Question: 'Would you mind verifying patient insurance coverage?'\n",
      "Response: 'what do you like to verify'\n",
      "Raw LLM response: 'True\n",
      "'\n",
      "Cleaned result: 'TRUE'\n",
      "Returning True\n",
      "Consent: True\n",
      "Consent received - starting verification\n",
      "Response: What is the patient's current eligibility status?, Is transition: True\n",
      "\n",
      "Responding: What is the patient's current eligibility status?\n",
      " > Text splitted to sentences.\n",
      "[\"What is the patient's current eligibility status?\"]\n",
      " > Processing time: 7.06561803817749\n",
      " > Real-time factor: 1.969121306140213\n",
      "\n",
      "Listening...\n",
      "Original input:  The patient is eligible and active.\n",
      "\n",
      "DEBUG - Validation:\n",
      "Input text: ' The patient is eligible and active.'\n",
      "Verification started: True\n",
      "Current context: insurance verification\n",
      "Current field: status\n",
      "Current question: What is the patient's current eligibility status?\n",
      "Contains relevant info for status: True\n",
      "Input valid: Contains relevant information\n",
      "Processing response:  The patient is eligible and active.\n",
      "\n",
      "=== DEBUG - Process Response Start ===\n",
      "Input text: ' The patient is eligible and active.'\n",
      "Current phase: Insurance Verification\n",
      "\n",
      "=== DEBUG - Insurance Verification Phase ===\n",
      "Current category: eligibility\n",
      "\n",
      "DEBUG - Status Extraction:\n",
      "Question: 'What is the patient's current eligibility status?'\n",
      "Response: ' The patient is eligible and active.'\n",
      "LLM status result: 'Active'\n",
      "Extracted status: Active\n",
      "Response: What is the effective date of coverage?, Is transition: False\n",
      "\n",
      "Responding: What is the effective date of coverage?\n",
      " > Text splitted to sentences.\n",
      "['What is the effective date of coverage?']\n",
      " > Processing time: 2.8233816623687744\n",
      " > Real-time factor: 0.949712680852323\n",
      "\n",
      "Listening...\n",
      "Original input:  The effective date of coverage is January 1st, 2025.\n",
      "\n",
      "DEBUG - Validation:\n",
      "Input text: ' The effective date of coverage is January 1st, 2025.'\n",
      "Verification started: True\n",
      "Current context: insurance verification\n",
      "Current field: effective_date\n",
      "Current question: What is the effective date of coverage?\n",
      "Contains relevant info for effective_date: True\n",
      "Input valid: Contains relevant information\n",
      "Processing response:  The effective date of coverage is January 1st, 2025.\n",
      "\n",
      "=== DEBUG - Process Response Start ===\n",
      "Input text: ' The effective date of coverage is January 1st, 2025.'\n",
      "Current phase: Insurance Verification\n",
      "\n",
      "=== DEBUG - Insurance Verification Phase ===\n",
      "Current category: eligibility\n",
      "\n",
      "DEBUG - Date Extraction:\n",
      "Question: 'What is the effective date of coverage?'\n",
      "Response: ' The effective date of coverage is January 1st, 2025.'\n",
      "LLM date result: '01/01/2025'\n",
      "Extracted effective_date: 01/01/2025\n",
      "Response: What type of plan does the patient have?, Is transition: False\n",
      "\n",
      "Responding: What type of plan does the patient have?\n",
      " > Text splitted to sentences.\n",
      "['What type of plan does the patient have?']\n",
      " > Processing time: 2.516221761703491\n",
      " > Real-time factor: 0.8953442074225727\n",
      "\n",
      "Listening...\n",
      "Original input:  The patient has BPO plan.\n",
      "\n",
      "DEBUG - Validation:\n",
      "Input text: ' The patient has BPO plan.'\n",
      "Verification started: True\n",
      "Current context: insurance verification\n",
      "Current field: plan_type\n",
      "Current question: What type of plan does the patient have?\n",
      "Contains relevant info for plan_type: True\n",
      "Input valid: Contains relevant information\n",
      "Processing response:  The patient has BPO plan.\n",
      "\n",
      "=== DEBUG - Process Response Start ===\n",
      "Input text: ' The patient has BPO plan.'\n",
      "Current phase: Insurance Verification\n",
      "\n",
      "=== DEBUG - Insurance Verification Phase ===\n",
      "Current category: eligibility\n",
      "\n",
      "DEBUG - Plan Type Extraction:\n",
      "Question: 'What type of plan does the patient have?'\n",
      "Response: ' The patient has BPO plan.'\n",
      "LLM plan type result: 'BPO'\n",
      "Extracted plan_type: BPO\n",
      "Response: Now for benefits. What is the annual maximum benefit?, Is transition: False\n",
      "\n",
      "Responding: Now for benefits. What is the annual maximum benefit?\n",
      " > Text splitted to sentences.\n",
      "['Now for benefits.', 'What is the annual maximum benefit?']\n",
      " > Processing time: 2.387190103530884\n",
      " > Real-time factor: 0.48365868295038217\n",
      "\n",
      "Listening...\n",
      "Original input:  and all maximum benefit is $1500.\n",
      "\n",
      "DEBUG - Validation:\n",
      "Input text: ' and all maximum benefit is $1500.'\n",
      "Verification started: True\n",
      "Current context: insurance verification\n",
      "Current field: annual_maximum\n",
      "Current question: What is the annual maximum benefit?\n",
      "Contains relevant info for annual_maximum: True\n",
      "Input valid: Contains relevant information\n",
      "Processing response:  and all maximum benefit is $1500.\n",
      "\n",
      "=== DEBUG - Process Response Start ===\n",
      "Input text: ' and all maximum benefit is $1500.'\n",
      "Current phase: Insurance Verification\n",
      "\n",
      "=== DEBUG - Insurance Verification Phase ===\n",
      "Current category: benefits\n",
      "\n",
      "DEBUG - Amount Extraction:\n",
      "Question: 'What is the annual maximum benefit?'\n",
      "Response: ' and all maximum benefit is $1500.'\n",
      "LLM amount result: '1500'\n",
      "Extracted annual_maximum: 1500.0\n",
      "Response: What is the remaining benefit amount?, Is transition: False\n",
      "\n",
      "Responding: What is the remaining benefit amount?\n",
      " > Text splitted to sentences.\n",
      "['What is the remaining benefit amount?']\n",
      " > Processing time: 2.234468460083008\n",
      " > Real-time factor: 0.8118578556688361\n",
      "\n",
      "Listening...\n",
      "Original input:  The remaining benefit amount is $250.\n",
      "\n",
      "DEBUG - Validation:\n",
      "Input text: ' The remaining benefit amount is $250.'\n",
      "Verification started: True\n",
      "Current context: insurance verification\n",
      "Current field: remaining_maximum\n",
      "Current question: What is the remaining benefit amount?\n",
      "Contains relevant info for remaining_maximum: True\n",
      "Input valid: Contains relevant information\n",
      "Processing response:  The remaining benefit amount is $250.\n",
      "\n",
      "=== DEBUG - Process Response Start ===\n",
      "Input text: ' The remaining benefit amount is $250.'\n",
      "Current phase: Insurance Verification\n",
      "\n",
      "=== DEBUG - Insurance Verification Phase ===\n",
      "Current category: benefits\n",
      "\n",
      "DEBUG - Amount Extraction:\n",
      "Question: 'What is the remaining benefit amount?'\n",
      "Response: ' The remaining benefit amount is $250.'\n",
      "LLM amount result: '250'\n",
      "Extracted remaining_maximum: 250.0\n",
      "Response: What is the deductible amount?, Is transition: False\n",
      "\n",
      "Responding: What is the deductible amount?\n",
      " > Text splitted to sentences.\n",
      "['What is the deductible amount?']\n",
      " > Processing time: 2.4661853313446045\n",
      " > Real-time factor: 0.9877104503804949\n",
      "\n",
      "Listening...\n",
      "Original input:  The amount is $50.\n",
      "\n",
      "DEBUG - Validation:\n",
      "Input text: ' The amount is $50.'\n",
      "Verification started: True\n",
      "Current context: insurance verification\n",
      "Current field: deductible\n",
      "Current question: What is the deductible amount?\n",
      "Contains relevant info for deductible: True\n",
      "Input valid: Contains relevant information\n",
      "Processing response:  The amount is $50.\n",
      "\n",
      "=== DEBUG - Process Response Start ===\n",
      "Input text: ' The amount is $50.'\n",
      "Current phase: Insurance Verification\n",
      "\n",
      "=== DEBUG - Insurance Verification Phase ===\n",
      "Current category: benefits\n",
      "\n",
      "DEBUG - Amount Extraction:\n",
      "Question: 'What is the deductible amount?'\n",
      "Response: ' The amount is $50.'\n",
      "LLM amount result: '50'\n",
      "Extracted deductible: 50.0\n",
      "Response: How much of the deductible has been met? Please provide dollar amount., Is transition: False\n",
      "\n",
      "Responding: How much of the deductible has been met? Please provide dollar amount.\n",
      " > Text splitted to sentences.\n",
      "['How much of the deductible has been met?', 'Please provide dollar amount.']\n",
      " > Processing time: 5.36470890045166\n",
      " > Real-time factor: 0.9807958945920594\n",
      "\n",
      "Listening...\n",
      "Original input:  $0.00 after deductible has been made.\n",
      "\n",
      "DEBUG - Validation:\n",
      "Input text: ' $0.00 after deductible has been made.'\n",
      "Verification started: True\n",
      "Current context: insurance verification\n",
      "Current field: deductible_met\n",
      "Current question: How much of the deductible has been met? Please provide dollar amount.\n",
      "Contains relevant info for deductible_met: True\n",
      "Input valid: Contains relevant information\n",
      "Processing response:  $0.00 after deductible has been made.\n",
      "\n",
      "=== DEBUG - Process Response Start ===\n",
      "Input text: ' $0.00 after deductible has been made.'\n",
      "Current phase: Insurance Verification\n",
      "\n",
      "=== DEBUG - Insurance Verification Phase ===\n",
      "Current category: benefits\n",
      "\n",
      "DEBUG - Amount Extraction:\n",
      "Question: 'How much of the deductible has been met? Please provide dollar amount.'\n",
      "Response: ' $0.00 after deductible has been made.'\n",
      "LLM amount result: '0.00'\n",
      "Extracted deductible_met: 0.0\n",
      "Response: What is the benefit period?, Is transition: False\n",
      "\n",
      "Responding: What is the benefit period?\n",
      " > Text splitted to sentences.\n",
      "['What is the benefit period?']\n",
      " > Processing time: 1.8439741134643555\n",
      " > Real-time factor: 0.8490567407678131\n",
      "\n",
      "Listening...\n",
      "Original input:  The benefit barrier is calendar year.\n",
      "\n",
      "DEBUG - Validation:\n",
      "Input text: ' The benefit barrier is calendar year.'\n",
      "Verification started: True\n",
      "Current context: insurance verification\n",
      "Current field: benefit_period\n",
      "Current question: What is the benefit period?\n",
      "Contains relevant info for benefit_period: True\n",
      "Input valid: Contains relevant information\n",
      "Processing response:  The benefit barrier is calendar year.\n",
      "\n",
      "=== DEBUG - Process Response Start ===\n",
      "Input text: ' The benefit barrier is calendar year.'\n",
      "Current phase: Insurance Verification\n",
      "\n",
      "=== DEBUG - Insurance Verification Phase ===\n",
      "Current category: benefits\n",
      "\n",
      "DEBUG - Period Extraction:\n",
      "Question: 'What is the benefit period?'\n",
      "Response: ' The benefit barrier is calendar year.'\n",
      "LLM period result: 'Calendar Year'\n",
      "Extracted benefit_period: Calendar Year\n",
      "Response: Let's check coverage percentages. What is the coverage percentage for preventive services?, Is transition: False\n",
      "\n",
      "Responding: Let's check coverage percentages. What is the coverage percentage for preventive services?\n",
      " > Text splitted to sentences.\n",
      "[\"Let's check coverage percentages.\", 'What is the coverage percentage for preventive services?']\n",
      " > Processing time: 8.10115933418274\n",
      " > Real-time factor: 1.198669766740454\n",
      "\n",
      "Listening...\n",
      "Original input:  It is 50%\n",
      "\n",
      "DEBUG - Validation:\n",
      "Input text: ' It is 50%'\n",
      "Verification started: True\n",
      "Current context: insurance verification\n",
      "Current field: preventive\n",
      "Current question: What is the coverage percentage for preventive services?\n",
      "Contains relevant info for preventive: True\n",
      "Input valid: Contains relevant information\n",
      "Processing response:  It is 50%\n",
      "\n",
      "=== DEBUG - Process Response Start ===\n",
      "Input text: ' It is 50%'\n",
      "Current phase: Insurance Verification\n",
      "\n",
      "=== DEBUG - Insurance Verification Phase ===\n",
      "Current category: coverage\n",
      "\n",
      "DEBUG - Percentage Extraction:\n",
      "Question: 'What is the coverage percentage for preventive services?'\n",
      "Response: ' It is 50%'\n",
      "LLM percentage result: '50'\n",
      "Extracted preventive: 50\n",
      "Response: What is the coverage percentage for basic services?, Is transition: False\n",
      "\n",
      "Responding: What is the coverage percentage for basic services?\n",
      " > Text splitted to sentences.\n",
      "['What is the coverage percentage for basic services?']\n",
      " > Processing time: 3.377725839614868\n",
      " > Real-time factor: 0.944396109295849\n",
      "\n",
      "Listening...\n",
      "Original input:  80%\n",
      "\n",
      "DEBUG - Validation:\n",
      "Input text: ' 80%'\n",
      "Verification started: True\n",
      "Current context: insurance verification\n",
      "Current field: basic\n",
      "Current question: What is the coverage percentage for basic services?\n",
      "Contains relevant info for basic: True\n",
      "Input valid: Contains relevant information\n",
      "Processing response:  80%\n",
      "\n",
      "=== DEBUG - Process Response Start ===\n",
      "Input text: ' 80%'\n",
      "Current phase: Insurance Verification\n",
      "\n",
      "=== DEBUG - Insurance Verification Phase ===\n",
      "Current category: coverage\n",
      "\n",
      "DEBUG - Percentage Extraction:\n",
      "Question: 'What is the coverage percentage for basic services?'\n",
      "Response: ' 80%'\n",
      "LLM percentage result: '80'\n",
      "Extracted basic: 80\n",
      "Response: What is the coverage percentage for major services?, Is transition: False\n",
      "\n",
      "Responding: What is the coverage percentage for major services?\n",
      " > Text splitted to sentences.\n",
      "['What is the coverage percentage for major services?']\n",
      " > Processing time: 4.734072685241699\n",
      " > Real-time factor: 1.2983370983778542\n",
      "\n",
      "Listening...\n",
      "Original input:  30%\n",
      "\n",
      "DEBUG - Validation:\n",
      "Input text: ' 30%'\n",
      "Verification started: True\n",
      "Current context: insurance verification\n",
      "Current field: major\n",
      "Current question: What is the coverage percentage for major services?\n",
      "Contains relevant info for major: True\n",
      "Input valid: Contains relevant information\n",
      "Processing response:  30%\n",
      "\n",
      "=== DEBUG - Process Response Start ===\n",
      "Input text: ' 30%'\n",
      "Current phase: Insurance Verification\n",
      "\n",
      "=== DEBUG - Insurance Verification Phase ===\n",
      "Current category: coverage\n",
      "\n",
      "DEBUG - Percentage Extraction:\n",
      "Question: 'What is the coverage percentage for major services?'\n",
      "Response: ' 30%'\n",
      "LLM percentage result: '30'\n",
      "Extracted major: 30\n",
      "Response: What is the coverage percentage for periodontal services?, Is transition: False\n",
      "\n",
      "Responding: What is the coverage percentage for periodontal services?\n",
      " > Text splitted to sentences.\n",
      "['What is the coverage percentage for periodontal services?']\n",
      " > Processing time: 3.5498054027557373\n",
      " > Real-time factor: 0.9235558940292147\n",
      "\n",
      "Listening...\n",
      "Original input:  75%\n",
      "\n",
      "DEBUG - Validation:\n",
      "Input text: ' 75%'\n",
      "Verification started: True\n",
      "Current context: insurance verification\n",
      "Current field: periodontics\n",
      "Current question: What is the coverage percentage for periodontal services?\n",
      "Contains relevant info for periodontics: True\n",
      "Input valid: Contains relevant information\n",
      "Processing response:  75%\n",
      "\n",
      "=== DEBUG - Process Response Start ===\n",
      "Input text: ' 75%'\n",
      "Current phase: Insurance Verification\n",
      "\n",
      "=== DEBUG - Insurance Verification Phase ===\n",
      "Current category: coverage\n",
      "\n",
      "DEBUG - Percentage Extraction:\n",
      "Question: 'What is the coverage percentage for periodontal services?'\n",
      "Response: ' 75%'\n",
      "LLM percentage result: '75'\n",
      "Extracted periodontics: 75\n",
      "Response: What is the coverage percentage for endodontic services?, Is transition: False\n",
      "\n",
      "Responding: What is the coverage percentage for endodontic services?\n",
      " > Text splitted to sentences.\n",
      "['What is the coverage percentage for endodontic services?']\n",
      " > Processing time: 3.6431782245635986\n",
      " > Real-time factor: 0.9421570633752504\n",
      "\n",
      "Listening...\n",
      "Original input:  25%\n",
      "\n",
      "DEBUG - Validation:\n",
      "Input text: ' 25%'\n",
      "Verification started: True\n",
      "Current context: insurance verification\n",
      "Current field: endodontics\n",
      "Current question: What is the coverage percentage for endodontic services?\n",
      "Contains relevant info for endodontics: True\n",
      "Input valid: Contains relevant information\n",
      "Processing response:  25%\n",
      "\n",
      "=== DEBUG - Process Response Start ===\n",
      "Input text: ' 25%'\n",
      "Current phase: Insurance Verification\n",
      "\n",
      "=== DEBUG - Insurance Verification Phase ===\n",
      "Current category: coverage\n",
      "\n",
      "DEBUG - Percentage Extraction:\n",
      "Question: 'What is the coverage percentage for endodontic services?'\n",
      "Response: ' 25%'\n",
      "LLM percentage result: '25'\n",
      "Extracted endodontics: 25\n",
      "Response: Finally, about limitations. Are there any waiting periods?, Is transition: False\n",
      "\n",
      "Responding: Finally, about limitations. Are there any waiting periods?\n",
      " > Text splitted to sentences.\n",
      "['Finally, about limitations.', 'Are there any waiting periods?']\n",
      " > Processing time: 4.94825005531311\n",
      " > Real-time factor: 0.9222757786689723\n",
      "\n",
      "Listening...\n",
      "Original input:  Yes, there is a waiting period of up to 6 months.\n",
      "\n",
      "DEBUG - Validation:\n",
      "Input text: ' Yes, there is a waiting period of up to 6 months.'\n",
      "Verification started: True\n",
      "Current context: insurance verification\n",
      "Current field: waiting_period\n",
      "Current question: Are there any waiting periods?\n",
      "Contains relevant info for waiting_period: True\n",
      "Input valid: Contains relevant information\n",
      "Processing response:  Yes, there is a waiting period of up to 6 months.\n",
      "\n",
      "=== DEBUG - Process Response Start ===\n",
      "Input text: ' Yes, there is a waiting period of up to 6 months.'\n",
      "Current phase: Insurance Verification\n",
      "\n",
      "=== DEBUG - Insurance Verification Phase ===\n",
      "Current category: limitations\n",
      "\n",
      "DEBUG - Period Extraction:\n",
      "Question: 'Are there any waiting periods?'\n",
      "Response: ' Yes, there is a waiting period of up to 6 months.'\n",
      "LLM period result: '6 months'\n",
      "Extracted waiting_period: 6 months\n",
      "Response: What are the frequency limitations?, Is transition: False\n",
      "\n",
      "Responding: What are the frequency limitations?\n",
      " > Text splitted to sentences.\n",
      "['What are the frequency limitations?']\n",
      " > Processing time: 2.196969747543335\n",
      " > Real-time factor: 0.7849880563477206\n",
      "\n",
      "Listening...\n",
      "Original input:  It is twice per year.\n",
      "\n",
      "DEBUG - Validation:\n",
      "Input text: ' It is twice per year.'\n",
      "Verification started: True\n",
      "Current context: insurance verification\n",
      "Current field: frequency\n",
      "Current question: What are the frequency limitations?\n",
      "Contains relevant info for frequency: True\n",
      "Input valid: Contains relevant information\n",
      "Processing response:  It is twice per year.\n",
      "\n",
      "=== DEBUG - Process Response Start ===\n",
      "Input text: ' It is twice per year.'\n",
      "Current phase: Insurance Verification\n",
      "\n",
      "=== DEBUG - Insurance Verification Phase ===\n",
      "Current category: limitations\n",
      "\n",
      "DEBUG - Frequency Extraction:\n",
      "Question: 'What are the frequency limitations?'\n",
      "Response: ' It is twice per year.'\n",
      "LLM frequency result: '```json\n",
      "{\"\": \"twice per year\"}\n",
      "```'\n",
      "Extracted frequency: {'Original Response': ' It is twice per year.'}\n",
      "Response: Is there a missing tooth clause?, Is transition: False\n",
      "\n",
      "Responding: Is there a missing tooth clause?\n",
      " > Text splitted to sentences.\n",
      "['Is there a missing tooth clause?']\n",
      " > Processing time: 3.21445631980896\n",
      " > Real-time factor: 1.2301923398324695\n",
      "\n",
      "Listening...\n",
      "Original input:  Yes, that is.\n",
      "\n",
      "DEBUG - Validation:\n",
      "Input text: ' Yes, that is.'\n",
      "Verification started: True\n",
      "Current context: insurance verification\n",
      "Current field: missing_tooth\n",
      "Current question: Is there a missing tooth clause?\n",
      "Contains relevant info for missing_tooth: True\n",
      "Input valid: Contains relevant information\n",
      "Processing response:  Yes, that is.\n",
      "\n",
      "=== DEBUG - Process Response Start ===\n",
      "Input text: ' Yes, that is.'\n",
      "Current phase: Insurance Verification\n",
      "\n",
      "=== DEBUG - Insurance Verification Phase ===\n",
      "Current category: limitations\n",
      "\n",
      "DEBUG - Boolean Extraction:\n",
      "Question: ' Yes, that is.'\n",
      "Response: 'Is there a missing tooth clause?'\n",
      "Raw LLM response: 'True\n",
      "'\n",
      "Cleaned result: 'TRUE'\n",
      "Returning True\n",
      "Extracted missing_tooth: True\n",
      "Response: Are there any pre-authorization requirements?, Is transition: False\n",
      "\n",
      "Responding: Are there any pre-authorization requirements?\n",
      " > Text splitted to sentences.\n",
      "['Are there any pre-authorization requirements?']\n",
      " > Processing time: 3.531045436859131\n",
      " > Real-time factor: 1.04134859676257\n",
      "\n",
      "Listening...\n",
      "Original input:  No pre-authorization requirements are needed.\n",
      "\n",
      "DEBUG - Validation:\n",
      "Input text: ' No pre-authorization requirements are needed.'\n",
      "Verification started: True\n",
      "Current context: insurance verification\n",
      "Current field: pre_authorization\n",
      "Current question: Are there any pre-authorization requirements?\n",
      "Contains relevant info for pre_authorization: True\n",
      "Input valid: Contains relevant information\n",
      "Processing response:  No pre-authorization requirements are needed.\n",
      "\n",
      "=== DEBUG - Process Response Start ===\n",
      "Input text: ' No pre-authorization requirements are needed.'\n",
      "Current phase: Insurance Verification\n",
      "\n",
      "=== DEBUG - Insurance Verification Phase ===\n",
      "Current category: limitations\n",
      "\n",
      "DEBUG - Boolean Extraction:\n",
      "Question: ' No pre-authorization requirements are needed.'\n",
      "Response: 'Are there any pre-authorization requirements?'\n",
      "Raw LLM response: 'False\n",
      "'\n",
      "Cleaned result: 'FALSE'\n",
      "Returning False\n",
      "Extracted pre_authorization: False\n",
      "Response: Verification complete!, Is transition: False\n",
      "\n",
      "Responding: Verification complete!\n",
      " > Text splitted to sentences.\n",
      "['Verification complete!']\n",
      " > Processing time: 1.634274959564209\n",
      " > Real-time factor: 0.7689433863603364\n",
      "\n",
      "Verification Complete!\n",
      "\n",
      "Final Verification Summary:\n",
      "\n",
      "ELIGIBILITY:\n",
      "  status: Active\n",
      "  effective_date: 01/01/2025\n",
      "  plan_type: BPO\n",
      "\n",
      "BENEFITS:\n",
      "  annual_maximum: 1500.0\n",
      "  remaining_maximum: 250.0\n",
      "  deductible: 50.0\n",
      "  deductible_met: 0.0\n",
      "  benefit_period: Calendar Year\n",
      "\n",
      "COVERAGE:\n",
      "  preventive: 50\n",
      "  basic: 80\n",
      "  major: 30\n",
      "  periodontics: 75\n",
      "  endodontics: 25\n",
      "\n",
      "LIMITATIONS:\n",
      "  waiting_period: 6 months\n",
      "  frequency: {'Original Response': ' It is twice per year.'}\n",
      "  missing_tooth: True\n",
      "  pre_authorization: False\n",
      " > Text splitted to sentences.\n",
      "['All verification information has been collected.', 'Thank you for your help.']\n",
      " > Processing time: 3.5704689025878906\n",
      " > Real-time factor: 0.6405301297030639\n",
      "\n",
      "Session ended\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import json\n",
    "import warnings\n",
    "import nltk\n",
    "import time\n",
    "import speech_recognition as sr\n",
    "import whisper  # Added this import\n",
    "\n",
    "from utils import (\n",
    "    fake_patient, \n",
    "    format_speech_output, \n",
    "    get_verification_summary, \n",
    "    initialize_correction_system,\n",
    "    validate_input_context,\n",
    "    enhance_accent_handling,\n",
    "    handle_confirmation\n",
    ")\n",
    "from llm import initialize_llm\n",
    "from stt import initialize_enhanced_recognition, listen_for_speech\n",
    "from tts import initialize_tts, handle_speech_output\n",
    "from verification import InsuranceVerification\n",
    "from flow import ConversationFlowManager, verify_patient\n",
    "\n",
    "def main():\n",
    "   current_play_obj = None  # Initialize at the start\n",
    "   try:\n",
    "       nltk.download('punkt_tab', quiet=True)\n",
    "       warnings.filterwarnings('ignore')\n",
    "       office_name = \"Everest Dental Clinic\"\n",
    "       patient = fake_patient()\n",
    "       verification = InsuranceVerification(office_name, patient)\n",
    "       flow_manager = ConversationFlowManager(verification, patient)\n",
    "       faiss_index, correction_df = initialize_correction_system()\n",
    "       \n",
    "       if faiss_index is None or correction_df is None:\n",
    "           print(\"Could not initialize correction system\")\n",
    "           return\n",
    "\n",
    "       tts = initialize_tts()\n",
    "       recognizer = initialize_enhanced_recognition()\n",
    "       queue = []\n",
    "       play_obj = None\n",
    "\n",
    "       if not hasattr(whisper, 'cached_model'):\n",
    "           whisper.cached_model = whisper.load_model(\"base\")\n",
    "\n",
    "       # Display patient information\n",
    "       print(\"\\nPatient Information Available:\")\n",
    "       print(f\"Name: {patient['first_name']} {patient['last_name']}\")\n",
    "       print(f\"DOB: {patient['date_of_birth']}\")\n",
    "       print(f\"Member ID: {patient['member_number']}\")\n",
    "       print(f\"Insurance: {patient['insurance_provider']}\")\n",
    "       print(\"\\nStarting in Patient Information Phase\\n\")\n",
    "\n",
    "       # Initial greeting\n",
    "       initial_message = (\n",
    "           f\"Hi, this is an assistant from {office_name}. \"\n",
    "           f\"I'm calling about our patient {patient['first_name']} {patient['last_name']}. \"\n",
    "           f\"Would you mind helping me verify insurance coverage?\"\n",
    "       )\n",
    "       initial_message = format_speech_output(initial_message)\n",
    "       wav = np.array(tts.tts(text=initial_message))\n",
    "       current_play_obj = handle_speech_output(queue, play_obj, wav, recognizer, None)\n",
    "       play_obj = current_play_obj\n",
    "\n",
    "       # Main conversation loop\n",
    "       while True:\n",
    "           try:\n",
    "               with sr.Microphone() as source:\n",
    "                   # Handle interruptions during speech\n",
    "                   if current_play_obj and current_play_obj.is_playing():\n",
    "                       if recognizer.get_energy() > 3000:\n",
    "                           current_play_obj.stop()\n",
    "                           queue.clear()\n",
    "                           audio = listen_for_speech(recognizer, source, current_play_obj)\n",
    "                           if audio:\n",
    "                               text = recognizer.recognize_whisper(audio, model=\"base\")\n",
    "                               print(f\"Heard during speech: {text}\")\n",
    "                               continue\n",
    "\n",
    "                   # Process queued audio\n",
    "                   while len(queue) > 0:\n",
    "                       if current_play_obj and current_play_obj.is_playing():\n",
    "                           time.sleep(0.1)\n",
    "                           continue\n",
    "                       current_play_obj = queue.pop(0).play()\n",
    "                       play_obj = current_play_obj\n",
    "\n",
    "                   # Listen for speech\n",
    "                   recognizer.adjust_for_ambient_noise(source, duration=1)\n",
    "                   audio = listen_for_speech(recognizer, source, current_play_obj)\n",
    "                   \n",
    "                   if not audio:\n",
    "                       continue\n",
    "\n",
    "                   # Process speech input\n",
    "                   text = recognizer.recognize_whisper(audio, model=\"base\")\n",
    "                   print(f\"Original input: {text}\")\n",
    "                   \n",
    "                   # Check for quit command\n",
    "                   if text.lower() == \"quit\":\n",
    "                       verification_summary = get_verification_summary(verification)\n",
    "                       print(\"\\nVerification Summary:\")\n",
    "                       for category, data in verification_summary['collected'].items():\n",
    "                           if data:\n",
    "                               print(f\"\\n{category.upper()}:\")\n",
    "                               for field, value in data.items():\n",
    "                                   print(f\"  {field}: {value}\")\n",
    "                       \n",
    "                       with open('verification_results.json', 'w') as f:\n",
    "                           json.dump(verification_summary, f, indent=2)\n",
    "                       print(\"\\nResults saved to verification_results.json\")\n",
    "                       break\n",
    "\n",
    "                   # First check if input needs correction in context\n",
    "                   validation_result = validate_input_context(text, verification, faiss_index, correction_df)\n",
    "                   \n",
    "                   if \"confidence_too_low\" in validation_result:\n",
    "                       print(f\"Low confidence: {validation_result['reason']}\")\n",
    "                       error_msg = \"I didn't quite catch that. Could you please repeat?\"\n",
    "                       wav = np.array(tts.tts(text=error_msg))\n",
    "                       current_play_obj = handle_speech_output(queue, play_obj, wav, recognizer, source)\n",
    "                       play_obj = current_play_obj\n",
    "                       continue\n",
    "                       \n",
    "                   if validation_result[\"needs_correction\"]:\n",
    "                       print(f\"Input needs correction: {validation_result['reason']}\")\n",
    "                       current_context = \"insurance verification\" if flow_manager.verification_started else \"patient information\"\n",
    "                       correction_result = enhance_accent_handling(text, faiss_index, correction_df, verification, current_context)\n",
    "                       \n",
    "                       if correction_result['needs_confirmation']:\n",
    "                           confirmation_msg = correction_result['confirmation_msg']\n",
    "                           formatted_confirmation = format_speech_output(confirmation_msg)\n",
    "                           print(f\"Seeking confirmation: {formatted_confirmation}\")\n",
    "                           wav = np.array(tts.tts(text=formatted_confirmation))\n",
    "                           current_play_obj = handle_speech_output(queue, play_obj, wav, recognizer, source)\n",
    "                           play_obj = current_play_obj\n",
    "                           \n",
    "                           confirmation_audio = listen_for_speech(recognizer, source, current_play_obj)\n",
    "                           if confirmation_audio:\n",
    "                               confirmation = recognizer.recognize_whisper(confirmation_audio, model=\"base\")\n",
    "                               if handle_confirmation(confirmation):\n",
    "                                   text = correction_result['corrected']\n",
    "                                   print(f\"Confirmation received, using corrected input: {text}\")\n",
    "                               else:\n",
    "                                   print(\"Correction rejected, asking for rephrasing\")\n",
    "                                   error_msg = \"Could you please rephrase that?\"\n",
    "                                   wav = np.array(tts.tts(text=error_msg))\n",
    "                                   current_play_obj = handle_speech_output(queue, play_obj, wav, recognizer, source)\n",
    "                                   play_obj = current_play_obj\n",
    "                                   continue\n",
    "                       else:\n",
    "                           text = correction_result['corrected']\n",
    "                           print(f\"Auto-corrected input: {text}\")\n",
    "                   else:\n",
    "                       print(f\"Input valid: {validation_result['reason']}\")\n",
    "\n",
    "                   # Process response and get next action\n",
    "                   print(f\"Processing response: {text}\")\n",
    "                   response, is_transition = flow_manager.process_response(text, verification)\n",
    "                   print(f\"Response: {response}, Is transition: {is_transition}\")\n",
    "                   \n",
    "                   if response:\n",
    "                       formatted_response = format_speech_output(response)\n",
    "                       print(f\"\\nResponding: {formatted_response}\")\n",
    "                       wav = np.array(tts.tts(text=formatted_response))\n",
    "                       current_play_obj = handle_speech_output(queue, play_obj, wav, recognizer, source)\n",
    "                       play_obj = current_play_obj\n",
    "\n",
    "                   # Check for verification completion\n",
    "                   verification_summary = get_verification_summary(verification)\n",
    "                   if verification_summary['status'] == 'complete':\n",
    "                       print(\"\\nVerification Complete!\")\n",
    "                       print(\"\\nFinal Verification Summary:\")\n",
    "                       for category, data in verification_summary['collected'].items():\n",
    "                           if data:\n",
    "                               print(f\"\\n{category.upper()}:\")\n",
    "                               for field, value in data.items():\n",
    "                                   print(f\"  {field}: {value}\")\n",
    "                       \n",
    "                       completion_message = \"All verification information has been collected. Thank you for your help.\"\n",
    "                       wav = np.array(tts.tts(text=completion_message))\n",
    "                       current_play_obj = handle_speech_output(queue, play_obj, wav, recognizer, source)\n",
    "                       play_obj = current_play_obj\n",
    "                       break\n",
    "\n",
    "           except sr.UnknownValueError:\n",
    "               print(\"Could not understand audio input\")\n",
    "               error_msg = \"I'm sorry, I couldn't understand that. Could you please repeat?\"\n",
    "               wav = np.array(tts.tts(text=error_msg))\n",
    "               current_play_obj = handle_speech_output(queue, play_obj, wav, recognizer, source)\n",
    "               play_obj = current_play_obj\n",
    "               \n",
    "           except Exception as e:\n",
    "               print(f\"Error in main loop: {str(e)}\")\n",
    "               error_msg = \"I encountered an error. Could you please rephrase that?\"\n",
    "               wav = np.array(tts.tts(text=error_msg))\n",
    "               current_play_obj = handle_speech_output(queue, play_obj, wav, recognizer, source)\n",
    "               play_obj = current_play_obj\n",
    "\n",
    "   finally:\n",
    "       if current_play_obj and current_play_obj.is_playing():\n",
    "           current_play_obj.stop()\n",
    "       if hasattr(whisper, 'cached_model'):\n",
    "           del whisper.cached_model\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    try:\n",
    "        main()\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\nExiting gracefully...\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\nAn error occurred: {str(e)}\")\n",
    "    finally:\n",
    "        print(\"\\nSession ended\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
